{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Script Generation\n",
    "In this project, you'll generate your own [Simpsons](https://en.wikipedia.org/wiki/The_Simpsons) TV scripts using RNNs.  You'll be using part of the [Simpsons dataset](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data) of scripts from 27 seasons.  The Neural Network you'll build will generate a new TV script for a scene at [Moe's Tavern](https://simpsonswiki.com/wiki/Moe's_Tavern).\n",
    "## Get the Data\n",
    "The data is already provided for you.  You'll be using a subset of the original dataset.  It consists of only the scenes in Moe's Tavern.  This doesn't include other versions of the tavern, like \"Moe's Cavern\", \"Flaming Moe's\", \"Uncle Moe's Family Feed-Bag\", etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "\n",
    "data_dir = './data/simpsons/moes_tavern_lines.txt'\n",
    "text = helper.load_data(data_dir)\n",
    "# Ignore notice, since we don't use it for analysing the data\n",
    "text = text[81:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "Play around with `view_sentence_range` to view different parts of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 11492\n",
      "Number of scenes: 262\n",
      "Average number of sentences in each scene: 15.248091603053435\n",
      "Number of lines: 4257\n",
      "Average number of words in each line: 11.50434578341555\n",
      "\n",
      "The sentences 0 to 10:\n",
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n",
      "Moe_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?\n",
      "Moe_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.\n",
      "Moe_Szyslak: What's the matter Homer? You're not your normal effervescent self.\n",
      "Homer_Simpson: I got my problems, Moe. Give me another one.\n",
      "Moe_Szyslak: Homer, hey, you should not drink to forget your problems.\n",
      "Barney_Gumble: Yeah, you should only drink to enhance your social skills.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocessing Functions\n",
    "The first thing to do to any dataset is preprocessing.  Implement the following preprocessing functions below:\n",
    "- Lookup Table\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### Lookup Table\n",
    "To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following tuple `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_to_int = {s:i for i, s in enumerate(set(text))}\n",
    "    int_to_vocab = dict(enumerate(set(text)))\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word \"bye\" and \"bye!\".\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( . )\n",
    "- Comma ( , )\n",
    "- Quotation Mark ( \" )\n",
    "- Semicolon ( ; )\n",
    "- Exclamation mark ( ! )\n",
    "- Question mark ( ? )\n",
    "- Left Parentheses ( ( )\n",
    "- Right Parentheses ( ) )\n",
    "- Dash ( -- )\n",
    "- Return ( \\n )\n",
    "\n",
    "This dictionary will be used to token the symbols and add the delimiter (space) around it.  This separates the symbols as it's own word, making it easier for the neural network to predict on the next word. Make sure you don't use a token that could be confused as a word. Instead of using the token \"dash\", try using something like \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['$', ')', '/', '-', '#', '(', ';', '\"', '&', '.', '_', \"'\", '!', '?', ',', '\\n', ':', '%']\n"
     ]
    }
   ],
   "source": [
    "# Find all non-alphanumeric and non-space chars in the text.\n",
    "import re\n",
    "specialChars = list(set(re.findall(\"[^A-Za-z0-9À-ÿ\\b \\b]\", text))) # \\b \\b is for single space between words.\n",
    "print(specialChars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # Unit test does not allow some of the symbols found in the text\n",
    "    # so I simply use the ones from the list above...\n",
    "    symbol_to_token = { '\"': \"||Double_Quote||\",\n",
    "                        #'&': \"||Ampersand||\",\n",
    "                        #'_': \"||Underscore||\",\n",
    "                        ')': \"||Right_Parantheses||\",\n",
    "                        ',': \"||Comma||\",\n",
    "                        '?': \"||Question_Mark||\",\n",
    "                        #'%': \"||Percent||\",\n",
    "                        #'$': \"||Dollar||\",\n",
    "                        #'/': \"||Slash||\",\n",
    "                        '(': \"||Left_Parantheses||\",\n",
    "                        '.': \"||Period||\",\n",
    "                        '\\n': \"||Line_Break||\",\n",
    "                        #'#': \"||Hash||\",\n",
    "                        '!': \"||Exclamation_Mark||\",\n",
    "                        #\"'\": \"||Single_Quote||\",\n",
    "                        ';': \"||Semi_Colon||\",\n",
    "                        #':': \"||Colon||\",\n",
    "                        #'-': \"||Dash||\",\n",
    "                        '--': \"||Double_Dash||\"\n",
    "                      }\n",
    "    return symbol_to_token\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "You'll build the components necessary to build a RNN by implementing the following functions below:\n",
    "- get_inputs\n",
    "- get_init_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### Check the Version of TensorFlow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paul/Software/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:14: UserWarning: No GPU found. Please use a GPU to train your neural network.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "Implement the `get_inputs()` function to create TF Placeholders for the Neural Network.  It should create the following placeholders:\n",
    "- Input text placeholder named \"input\" using the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) `name` parameter.\n",
    "- Targets placeholder\n",
    "- Learning Rate placeholder\n",
    "\n",
    "Return the placeholders in the following tuple `(Input, Targets, LearningRate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    i = tf.placeholder(tf.int32, shape=[None, None], name='input')\n",
    "    t = tf.placeholder(tf.int32, shape=[None, None], name='targets')\n",
    "    l = tf.placeholder(tf.float32, name='learningRate')\n",
    "    return i, t, l\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN Cell and Initialize\n",
    "Stack one or more [`BasicLSTMCells`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell) in a [`MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell).\n",
    "- The Rnn size should be set using `rnn_size`\n",
    "- Initalize Cell State using the MultiRNNCell's [`zero_state()`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell#zero_state) function\n",
    "    - Apply the name \"initial_state\" to the initial state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the cell and initial state in the following tuple `(Cell, InitialState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs (number of units in hidden layer)\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    # Tensorflow 1.1 syntax.\n",
    "    def build_cell(rnn_size):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        #drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return lstm\n",
    "    n_layers = 1\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(rnn_size) for _ in range(n_layers)])\n",
    "    \n",
    "    # Tensorflow 1.0 syntax.\n",
    "    '''\n",
    "    # Create basic cell.\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    \n",
    "    # Stack rnn_size basic cells.\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm]*1)\n",
    "    '''\n",
    "    \n",
    "    # Set initial state.\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    # Add name to initial state.\n",
    "    initial_state = tf.identity(initial_state, name='initial_state')\n",
    "    \n",
    "    return cell, initial_state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    # Create embedding lookup table.\n",
    "    # https://www.tensorflow.org/programmers_guide/embedding\n",
    "    word_embeddings = tf.get_variable('word_embeddings', [vocab_size, embed_dim])\n",
    "    embedded_word_ids = tf.nn.embedding_lookup(word_embeddings, input_data)\n",
    "    # TODO: Implement Function\n",
    "    return embedded_word_ids\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN\n",
    "You created a RNN Cell in the `get_init_cell()` function.  Time to use the cell to create a RNN.\n",
    "- Build the RNN using the [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n",
    " - Apply the name \"final_state\" to the final state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the outputs and final_state state in the following tuple `(Outputs, FinalState)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(state, name='final_state')\n",
    "    return outputs, final_state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "- Apply embedding to `input_data` using your `get_embed(input_data, vocab_size, embed_dim)` function.\n",
    "- Build RNN using `cell` and your `build_rnn(cell, inputs)` function.\n",
    "- Apply a fully connected layer with a linear activation and `vocab_size` as the number of outputs.\n",
    "\n",
    "Return the logits and final state in the following tuple (Logits, FinalState) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    embed = get_embed(input_data, vocab_size, rnn_size)\n",
    "    outputs, final_state = build_rnn(cell, embed)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "    \n",
    "    return logits, final_state\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches\n",
    "Implement `get_batches` to create batches of input and targets using `int_text`.  The batches should be a Numpy array with the shape `(number of batches, 2, batch size, sequence length)`. Each batch contains two elements:\n",
    "- The first element is a single batch of **input** with the shape `[batch size, sequence length]`\n",
    "- The second element is a single batch of **targets** with the shape `[batch size, sequence length]`\n",
    "\n",
    "If you can't fill the last batch with enough data, drop the last batch.\n",
    "\n",
    "For exmple, `get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 3, 2)` would return a Numpy array of the following:\n",
    "```\n",
    "[\n",
    "  # First Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 1  2], [ 7  8], [13 14]]\n",
    "    # Batch of targets\n",
    "    [[ 2  3], [ 8  9], [14 15]]\n",
    "  ]\n",
    "\n",
    "  # Second Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 3  4], [ 9 10], [15 16]]\n",
    "    # Batch of targets\n",
    "    [[ 4  5], [10 11], [16 17]]\n",
    "  ]\n",
    "\n",
    "  # Third Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 5  6], [11 12], [17 18]]\n",
    "    # Batch of targets\n",
    "    [[ 6  7], [12 13], [18  1]]\n",
    "  ]\n",
    "]\n",
    "```\n",
    "\n",
    "Notice that the last target value in the last batch is the first input value of the first batch. In this case, `1`. This is a common technique used when creating sequence batches, although it is rather unintuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    # Get number of batches.\n",
    "    chars_per_batch = batch_size * seq_length\n",
    "    n_batches = int(len(int_text) / chars_per_batch)\n",
    "    \n",
    "    # Truncate unused ids.\n",
    "    ids = int_text[:n_batches * chars_per_batch]\n",
    "    \n",
    "    # Reshape into rows of sequence length.\n",
    "    idsX = np.array(ids).reshape((-1, seq_length)).tolist()\n",
    "    idsY = np.roll(np.array(ids), -1).reshape((-1, seq_length)).tolist()\n",
    "    \n",
    "    # Append sequences into desired array shape.\n",
    "    idsAll = []\n",
    "    for b in range(n_batches):\n",
    "        batch = [[],[]]\n",
    "        for s in range(batch_size):\n",
    "            batch[0].append(idsX[s*n_batches+b])\n",
    "            batch[1].append(idsY[s*n_batches+b])\n",
    "        idsAll.append(batch)\n",
    "    idsAll = np.array(idsAll)\n",
    "\n",
    "    return idsAll\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "\n",
    "- Set `num_epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `embed_dim` to the size of the embedding.\n",
    "- Set `seq_length` to the length of sequence.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `show_every_n_batches` to the number of batches the neural network should print progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 10\n",
    "# Batch Size\n",
    "batch_size = 32\n",
    "# RNN Size\n",
    "rnn_size = 3\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 3\n",
    "# Sequence Length\n",
    "seq_length = 1\n",
    "# Learning Rate\n",
    "learning_rate = 0.01\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 10\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Train the neural network on the preprocessed data.  If you have a hard time getting a good loss, check the [forums](https://discussions.udacity.com/) to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/2159   train_loss = 8.822\n",
      "Epoch   0 Batch   10/2159   train_loss = 8.741\n",
      "Epoch   0 Batch   20/2159   train_loss = 8.563\n",
      "Epoch   0 Batch   30/2159   train_loss = 8.355\n",
      "Epoch   0 Batch   40/2159   train_loss = 8.275\n",
      "Epoch   0 Batch   50/2159   train_loss = 7.982\n",
      "Epoch   0 Batch   60/2159   train_loss = 7.724\n",
      "Epoch   0 Batch   70/2159   train_loss = 6.928\n",
      "Epoch   0 Batch   80/2159   train_loss = 6.951\n",
      "Epoch   0 Batch   90/2159   train_loss = 6.266\n",
      "Epoch   0 Batch  100/2159   train_loss = 5.670\n",
      "Epoch   0 Batch  110/2159   train_loss = 5.839\n",
      "Epoch   0 Batch  120/2159   train_loss = 6.323\n",
      "Epoch   0 Batch  130/2159   train_loss = 6.112\n",
      "Epoch   0 Batch  140/2159   train_loss = 6.059\n",
      "Epoch   0 Batch  150/2159   train_loss = 6.216\n",
      "Epoch   0 Batch  160/2159   train_loss = 5.925\n",
      "Epoch   0 Batch  170/2159   train_loss = 6.591\n",
      "Epoch   0 Batch  180/2159   train_loss = 7.185\n",
      "Epoch   0 Batch  190/2159   train_loss = 4.962\n",
      "Epoch   0 Batch  200/2159   train_loss = 6.633\n",
      "Epoch   0 Batch  210/2159   train_loss = 6.141\n",
      "Epoch   0 Batch  220/2159   train_loss = 6.188\n",
      "Epoch   0 Batch  230/2159   train_loss = 5.837\n",
      "Epoch   0 Batch  240/2159   train_loss = 6.238\n",
      "Epoch   0 Batch  250/2159   train_loss = 7.081\n",
      "Epoch   0 Batch  260/2159   train_loss = 5.697\n",
      "Epoch   0 Batch  270/2159   train_loss = 6.064\n",
      "Epoch   0 Batch  280/2159   train_loss = 6.373\n",
      "Epoch   0 Batch  290/2159   train_loss = 6.090\n",
      "Epoch   0 Batch  300/2159   train_loss = 6.243\n",
      "Epoch   0 Batch  310/2159   train_loss = 7.384\n",
      "Epoch   0 Batch  320/2159   train_loss = 6.199\n",
      "Epoch   0 Batch  330/2159   train_loss = 6.335\n",
      "Epoch   0 Batch  340/2159   train_loss = 6.611\n",
      "Epoch   0 Batch  350/2159   train_loss = 7.142\n",
      "Epoch   0 Batch  360/2159   train_loss = 5.552\n",
      "Epoch   0 Batch  370/2159   train_loss = 6.158\n",
      "Epoch   0 Batch  380/2159   train_loss = 5.783\n",
      "Epoch   0 Batch  390/2159   train_loss = 6.441\n",
      "Epoch   0 Batch  400/2159   train_loss = 6.373\n",
      "Epoch   0 Batch  410/2159   train_loss = 6.385\n",
      "Epoch   0 Batch  420/2159   train_loss = 6.150\n",
      "Epoch   0 Batch  430/2159   train_loss = 6.214\n",
      "Epoch   0 Batch  440/2159   train_loss = 6.443\n",
      "Epoch   0 Batch  450/2159   train_loss = 5.419\n",
      "Epoch   0 Batch  460/2159   train_loss = 6.319\n",
      "Epoch   0 Batch  470/2159   train_loss = 5.667\n",
      "Epoch   0 Batch  480/2159   train_loss = 5.547\n",
      "Epoch   0 Batch  490/2159   train_loss = 5.878\n",
      "Epoch   0 Batch  500/2159   train_loss = 5.366\n",
      "Epoch   0 Batch  510/2159   train_loss = 6.532\n",
      "Epoch   0 Batch  520/2159   train_loss = 5.319\n",
      "Epoch   0 Batch  530/2159   train_loss = 5.693\n",
      "Epoch   0 Batch  540/2159   train_loss = 6.459\n",
      "Epoch   0 Batch  550/2159   train_loss = 5.849\n",
      "Epoch   0 Batch  560/2159   train_loss = 6.675\n",
      "Epoch   0 Batch  570/2159   train_loss = 5.631\n",
      "Epoch   0 Batch  580/2159   train_loss = 6.669\n",
      "Epoch   0 Batch  590/2159   train_loss = 5.820\n",
      "Epoch   0 Batch  600/2159   train_loss = 6.159\n",
      "Epoch   0 Batch  610/2159   train_loss = 5.928\n",
      "Epoch   0 Batch  620/2159   train_loss = 5.625\n",
      "Epoch   0 Batch  630/2159   train_loss = 6.087\n",
      "Epoch   0 Batch  640/2159   train_loss = 5.439\n",
      "Epoch   0 Batch  650/2159   train_loss = 6.068\n",
      "Epoch   0 Batch  660/2159   train_loss = 6.091\n",
      "Epoch   0 Batch  670/2159   train_loss = 5.816\n",
      "Epoch   0 Batch  680/2159   train_loss = 6.272\n",
      "Epoch   0 Batch  690/2159   train_loss = 6.328\n",
      "Epoch   0 Batch  700/2159   train_loss = 5.287\n",
      "Epoch   0 Batch  710/2159   train_loss = 5.736\n",
      "Epoch   0 Batch  720/2159   train_loss = 5.638\n",
      "Epoch   0 Batch  730/2159   train_loss = 5.372\n",
      "Epoch   0 Batch  740/2159   train_loss = 5.575\n",
      "Epoch   0 Batch  750/2159   train_loss = 5.595\n",
      "Epoch   0 Batch  760/2159   train_loss = 5.669\n",
      "Epoch   0 Batch  770/2159   train_loss = 5.193\n",
      "Epoch   0 Batch  780/2159   train_loss = 5.314\n",
      "Epoch   0 Batch  790/2159   train_loss = 5.989\n",
      "Epoch   0 Batch  800/2159   train_loss = 6.671\n",
      "Epoch   0 Batch  810/2159   train_loss = 5.402\n",
      "Epoch   0 Batch  820/2159   train_loss = 5.821\n",
      "Epoch   0 Batch  830/2159   train_loss = 5.771\n",
      "Epoch   0 Batch  840/2159   train_loss = 5.766\n",
      "Epoch   0 Batch  850/2159   train_loss = 5.804\n",
      "Epoch   0 Batch  860/2159   train_loss = 4.904\n",
      "Epoch   0 Batch  870/2159   train_loss = 6.821\n",
      "Epoch   0 Batch  880/2159   train_loss = 6.058\n",
      "Epoch   0 Batch  890/2159   train_loss = 5.712\n",
      "Epoch   0 Batch  900/2159   train_loss = 5.710\n",
      "Epoch   0 Batch  910/2159   train_loss = 6.018\n",
      "Epoch   0 Batch  920/2159   train_loss = 5.142\n",
      "Epoch   0 Batch  930/2159   train_loss = 6.221\n",
      "Epoch   0 Batch  940/2159   train_loss = 5.914\n",
      "Epoch   0 Batch  950/2159   train_loss = 6.166\n",
      "Epoch   0 Batch  960/2159   train_loss = 4.992\n",
      "Epoch   0 Batch  970/2159   train_loss = 5.805\n",
      "Epoch   0 Batch  980/2159   train_loss = 5.451\n",
      "Epoch   0 Batch  990/2159   train_loss = 5.574\n",
      "Epoch   0 Batch 1000/2159   train_loss = 5.141\n",
      "Epoch   0 Batch 1010/2159   train_loss = 6.112\n",
      "Epoch   0 Batch 1020/2159   train_loss = 5.884\n",
      "Epoch   0 Batch 1030/2159   train_loss = 5.466\n",
      "Epoch   0 Batch 1040/2159   train_loss = 5.791\n",
      "Epoch   0 Batch 1050/2159   train_loss = 5.583\n",
      "Epoch   0 Batch 1060/2159   train_loss = 5.959\n",
      "Epoch   0 Batch 1070/2159   train_loss = 6.463\n",
      "Epoch   0 Batch 1080/2159   train_loss = 6.089\n",
      "Epoch   0 Batch 1090/2159   train_loss = 5.536\n",
      "Epoch   0 Batch 1100/2159   train_loss = 5.763\n",
      "Epoch   0 Batch 1110/2159   train_loss = 6.340\n",
      "Epoch   0 Batch 1120/2159   train_loss = 5.791\n",
      "Epoch   0 Batch 1130/2159   train_loss = 6.153\n",
      "Epoch   0 Batch 1140/2159   train_loss = 5.581\n",
      "Epoch   0 Batch 1150/2159   train_loss = 5.540\n",
      "Epoch   0 Batch 1160/2159   train_loss = 6.973\n",
      "Epoch   0 Batch 1170/2159   train_loss = 6.762\n",
      "Epoch   0 Batch 1180/2159   train_loss = 5.716\n",
      "Epoch   0 Batch 1190/2159   train_loss = 5.835\n",
      "Epoch   0 Batch 1200/2159   train_loss = 5.342\n",
      "Epoch   0 Batch 1210/2159   train_loss = 5.750\n",
      "Epoch   0 Batch 1220/2159   train_loss = 6.728\n",
      "Epoch   0 Batch 1230/2159   train_loss = 6.660\n",
      "Epoch   0 Batch 1240/2159   train_loss = 6.492\n",
      "Epoch   0 Batch 1250/2159   train_loss = 5.839\n",
      "Epoch   0 Batch 1260/2159   train_loss = 6.528\n",
      "Epoch   0 Batch 1270/2159   train_loss = 5.838\n",
      "Epoch   0 Batch 1280/2159   train_loss = 5.874\n",
      "Epoch   0 Batch 1290/2159   train_loss = 6.875\n",
      "Epoch   0 Batch 1300/2159   train_loss = 4.765\n",
      "Epoch   0 Batch 1310/2159   train_loss = 5.762\n",
      "Epoch   0 Batch 1320/2159   train_loss = 6.173\n",
      "Epoch   0 Batch 1330/2159   train_loss = 6.019\n",
      "Epoch   0 Batch 1340/2159   train_loss = 5.790\n",
      "Epoch   0 Batch 1350/2159   train_loss = 5.362\n",
      "Epoch   0 Batch 1360/2159   train_loss = 5.699\n",
      "Epoch   0 Batch 1370/2159   train_loss = 5.532\n",
      "Epoch   0 Batch 1380/2159   train_loss = 6.363\n",
      "Epoch   0 Batch 1390/2159   train_loss = 5.457\n",
      "Epoch   0 Batch 1400/2159   train_loss = 5.712\n",
      "Epoch   0 Batch 1410/2159   train_loss = 6.388\n",
      "Epoch   0 Batch 1420/2159   train_loss = 6.065\n",
      "Epoch   0 Batch 1430/2159   train_loss = 5.058\n",
      "Epoch   0 Batch 1440/2159   train_loss = 5.968\n",
      "Epoch   0 Batch 1450/2159   train_loss = 5.677\n",
      "Epoch   0 Batch 1460/2159   train_loss = 5.556\n",
      "Epoch   0 Batch 1470/2159   train_loss = 5.474\n",
      "Epoch   0 Batch 1480/2159   train_loss = 5.055\n",
      "Epoch   0 Batch 1490/2159   train_loss = 6.159\n",
      "Epoch   0 Batch 1500/2159   train_loss = 5.005\n",
      "Epoch   0 Batch 1510/2159   train_loss = 5.906\n",
      "Epoch   0 Batch 1520/2159   train_loss = 6.395\n",
      "Epoch   0 Batch 1530/2159   train_loss = 5.792\n",
      "Epoch   0 Batch 1540/2159   train_loss = 6.506\n",
      "Epoch   0 Batch 1550/2159   train_loss = 5.713\n",
      "Epoch   0 Batch 1560/2159   train_loss = 6.752\n",
      "Epoch   0 Batch 1570/2159   train_loss = 5.682\n",
      "Epoch   0 Batch 1580/2159   train_loss = 6.292\n",
      "Epoch   0 Batch 1590/2159   train_loss = 5.120\n",
      "Epoch   0 Batch 1600/2159   train_loss = 5.598\n",
      "Epoch   0 Batch 1610/2159   train_loss = 6.929\n",
      "Epoch   0 Batch 1620/2159   train_loss = 6.174\n",
      "Epoch   0 Batch 1630/2159   train_loss = 4.587\n",
      "Epoch   0 Batch 1640/2159   train_loss = 5.264\n",
      "Epoch   0 Batch 1650/2159   train_loss = 6.147\n",
      "Epoch   0 Batch 1660/2159   train_loss = 5.107\n",
      "Epoch   0 Batch 1670/2159   train_loss = 5.926\n",
      "Epoch   0 Batch 1680/2159   train_loss = 5.514\n",
      "Epoch   0 Batch 1690/2159   train_loss = 5.179\n",
      "Epoch   0 Batch 1700/2159   train_loss = 6.822\n",
      "Epoch   0 Batch 1710/2159   train_loss = 5.457\n",
      "Epoch   0 Batch 1720/2159   train_loss = 5.630\n",
      "Epoch   0 Batch 1730/2159   train_loss = 6.051\n",
      "Epoch   0 Batch 1740/2159   train_loss = 4.557\n",
      "Epoch   0 Batch 1750/2159   train_loss = 5.281\n",
      "Epoch   0 Batch 1760/2159   train_loss = 6.067\n",
      "Epoch   0 Batch 1770/2159   train_loss = 6.009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch 1780/2159   train_loss = 5.421\n",
      "Epoch   0 Batch 1790/2159   train_loss = 4.949\n",
      "Epoch   0 Batch 1800/2159   train_loss = 5.042\n",
      "Epoch   0 Batch 1810/2159   train_loss = 5.209\n",
      "Epoch   0 Batch 1820/2159   train_loss = 5.562\n",
      "Epoch   0 Batch 1830/2159   train_loss = 5.606\n",
      "Epoch   0 Batch 1840/2159   train_loss = 5.582\n",
      "Epoch   0 Batch 1850/2159   train_loss = 5.490\n",
      "Epoch   0 Batch 1860/2159   train_loss = 6.288\n",
      "Epoch   0 Batch 1870/2159   train_loss = 6.132\n",
      "Epoch   0 Batch 1880/2159   train_loss = 5.755\n",
      "Epoch   0 Batch 1890/2159   train_loss = 5.887\n",
      "Epoch   0 Batch 1900/2159   train_loss = 6.965\n",
      "Epoch   0 Batch 1910/2159   train_loss = 5.228\n",
      "Epoch   0 Batch 1920/2159   train_loss = 5.444\n",
      "Epoch   0 Batch 1930/2159   train_loss = 5.841\n",
      "Epoch   0 Batch 1940/2159   train_loss = 6.300\n",
      "Epoch   0 Batch 1950/2159   train_loss = 4.900\n",
      "Epoch   0 Batch 1960/2159   train_loss = 6.303\n",
      "Epoch   0 Batch 1970/2159   train_loss = 6.514\n",
      "Epoch   0 Batch 1980/2159   train_loss = 5.099\n",
      "Epoch   0 Batch 1990/2159   train_loss = 5.374\n",
      "Epoch   0 Batch 2000/2159   train_loss = 5.395\n",
      "Epoch   0 Batch 2010/2159   train_loss = 4.485\n",
      "Epoch   0 Batch 2020/2159   train_loss = 5.768\n",
      "Epoch   0 Batch 2030/2159   train_loss = 5.977\n",
      "Epoch   0 Batch 2040/2159   train_loss = 5.804\n",
      "Epoch   0 Batch 2050/2159   train_loss = 4.499\n",
      "Epoch   0 Batch 2060/2159   train_loss = 5.259\n",
      "Epoch   0 Batch 2070/2159   train_loss = 5.100\n",
      "Epoch   0 Batch 2080/2159   train_loss = 5.485\n",
      "Epoch   0 Batch 2090/2159   train_loss = 5.298\n",
      "Epoch   0 Batch 2100/2159   train_loss = 7.232\n",
      "Epoch   0 Batch 2110/2159   train_loss = 6.111\n",
      "Epoch   0 Batch 2120/2159   train_loss = 5.904\n",
      "Epoch   0 Batch 2130/2159   train_loss = 4.643\n",
      "Epoch   0 Batch 2140/2159   train_loss = 5.268\n",
      "Epoch   0 Batch 2150/2159   train_loss = 5.396\n",
      "Epoch   1 Batch    1/2159   train_loss = 5.166\n",
      "Epoch   1 Batch   11/2159   train_loss = 5.203\n",
      "Epoch   1 Batch   21/2159   train_loss = 4.938\n",
      "Epoch   1 Batch   31/2159   train_loss = 5.231\n",
      "Epoch   1 Batch   41/2159   train_loss = 4.889\n",
      "Epoch   1 Batch   51/2159   train_loss = 5.331\n",
      "Epoch   1 Batch   61/2159   train_loss = 6.030\n",
      "Epoch   1 Batch   71/2159   train_loss = 4.746\n",
      "Epoch   1 Batch   81/2159   train_loss = 5.713\n",
      "Epoch   1 Batch   91/2159   train_loss = 5.329\n",
      "Epoch   1 Batch  101/2159   train_loss = 5.593\n",
      "Epoch   1 Batch  111/2159   train_loss = 5.392\n",
      "Epoch   1 Batch  121/2159   train_loss = 6.133\n",
      "Epoch   1 Batch  131/2159   train_loss = 5.058\n",
      "Epoch   1 Batch  141/2159   train_loss = 4.463\n",
      "Epoch   1 Batch  151/2159   train_loss = 5.907\n",
      "Epoch   1 Batch  161/2159   train_loss = 5.156\n",
      "Epoch   1 Batch  171/2159   train_loss = 5.502\n",
      "Epoch   1 Batch  181/2159   train_loss = 5.242\n",
      "Epoch   1 Batch  191/2159   train_loss = 5.282\n",
      "Epoch   1 Batch  201/2159   train_loss = 4.519\n",
      "Epoch   1 Batch  211/2159   train_loss = 5.389\n",
      "Epoch   1 Batch  221/2159   train_loss = 6.261\n",
      "Epoch   1 Batch  231/2159   train_loss = 5.986\n",
      "Epoch   1 Batch  241/2159   train_loss = 4.978\n",
      "Epoch   1 Batch  251/2159   train_loss = 5.578\n",
      "Epoch   1 Batch  261/2159   train_loss = 5.064\n",
      "Epoch   1 Batch  271/2159   train_loss = 5.650\n",
      "Epoch   1 Batch  281/2159   train_loss = 5.610\n",
      "Epoch   1 Batch  291/2159   train_loss = 6.286\n",
      "Epoch   1 Batch  301/2159   train_loss = 5.474\n",
      "Epoch   1 Batch  311/2159   train_loss = 4.597\n",
      "Epoch   1 Batch  321/2159   train_loss = 6.423\n",
      "Epoch   1 Batch  331/2159   train_loss = 5.573\n",
      "Epoch   1 Batch  341/2159   train_loss = 5.937\n",
      "Epoch   1 Batch  351/2159   train_loss = 3.959\n",
      "Epoch   1 Batch  361/2159   train_loss = 5.095\n",
      "Epoch   1 Batch  371/2159   train_loss = 5.268\n",
      "Epoch   1 Batch  381/2159   train_loss = 5.517\n",
      "Epoch   1 Batch  391/2159   train_loss = 5.190\n",
      "Epoch   1 Batch  401/2159   train_loss = 4.741\n",
      "Epoch   1 Batch  411/2159   train_loss = 5.189\n",
      "Epoch   1 Batch  421/2159   train_loss = 5.214\n",
      "Epoch   1 Batch  431/2159   train_loss = 4.655\n",
      "Epoch   1 Batch  441/2159   train_loss = 6.196\n",
      "Epoch   1 Batch  451/2159   train_loss = 5.319\n",
      "Epoch   1 Batch  461/2159   train_loss = 4.354\n",
      "Epoch   1 Batch  471/2159   train_loss = 5.471\n",
      "Epoch   1 Batch  481/2159   train_loss = 4.434\n",
      "Epoch   1 Batch  491/2159   train_loss = 4.765\n",
      "Epoch   1 Batch  501/2159   train_loss = 5.457\n",
      "Epoch   1 Batch  511/2159   train_loss = 4.991\n",
      "Epoch   1 Batch  521/2159   train_loss = 5.148\n",
      "Epoch   1 Batch  531/2159   train_loss = 5.333\n",
      "Epoch   1 Batch  541/2159   train_loss = 4.416\n",
      "Epoch   1 Batch  551/2159   train_loss = 4.891\n",
      "Epoch   1 Batch  561/2159   train_loss = 6.102\n",
      "Epoch   1 Batch  571/2159   train_loss = 4.930\n",
      "Epoch   1 Batch  581/2159   train_loss = 5.211\n",
      "Epoch   1 Batch  591/2159   train_loss = 5.779\n",
      "Epoch   1 Batch  601/2159   train_loss = 5.218\n",
      "Epoch   1 Batch  611/2159   train_loss = 6.250\n",
      "Epoch   1 Batch  621/2159   train_loss = 4.988\n",
      "Epoch   1 Batch  631/2159   train_loss = 4.769\n",
      "Epoch   1 Batch  641/2159   train_loss = 5.168\n",
      "Epoch   1 Batch  651/2159   train_loss = 4.898\n",
      "Epoch   1 Batch  661/2159   train_loss = 5.549\n",
      "Epoch   1 Batch  671/2159   train_loss = 4.767\n",
      "Epoch   1 Batch  681/2159   train_loss = 5.652\n",
      "Epoch   1 Batch  691/2159   train_loss = 6.003\n",
      "Epoch   1 Batch  701/2159   train_loss = 6.004\n",
      "Epoch   1 Batch  711/2159   train_loss = 5.884\n",
      "Epoch   1 Batch  721/2159   train_loss = 6.027\n",
      "Epoch   1 Batch  731/2159   train_loss = 5.913\n",
      "Epoch   1 Batch  741/2159   train_loss = 4.990\n",
      "Epoch   1 Batch  751/2159   train_loss = 6.673\n",
      "Epoch   1 Batch  761/2159   train_loss = 4.446\n",
      "Epoch   1 Batch  771/2159   train_loss = 5.189\n",
      "Epoch   1 Batch  781/2159   train_loss = 4.719\n",
      "Epoch   1 Batch  791/2159   train_loss = 5.431\n",
      "Epoch   1 Batch  801/2159   train_loss = 5.137\n",
      "Epoch   1 Batch  811/2159   train_loss = 5.362\n",
      "Epoch   1 Batch  821/2159   train_loss = 6.210\n",
      "Epoch   1 Batch  831/2159   train_loss = 4.379\n",
      "Epoch   1 Batch  841/2159   train_loss = 4.647\n",
      "Epoch   1 Batch  851/2159   train_loss = 4.415\n",
      "Epoch   1 Batch  861/2159   train_loss = 4.538\n",
      "Epoch   1 Batch  871/2159   train_loss = 6.615\n",
      "Epoch   1 Batch  881/2159   train_loss = 4.860\n",
      "Epoch   1 Batch  891/2159   train_loss = 4.924\n",
      "Epoch   1 Batch  901/2159   train_loss = 5.833\n",
      "Epoch   1 Batch  911/2159   train_loss = 4.441\n",
      "Epoch   1 Batch  921/2159   train_loss = 5.166\n",
      "Epoch   1 Batch  931/2159   train_loss = 4.197\n",
      "Epoch   1 Batch  941/2159   train_loss = 4.856\n",
      "Epoch   1 Batch  951/2159   train_loss = 5.119\n",
      "Epoch   1 Batch  961/2159   train_loss = 5.497\n",
      "Epoch   1 Batch  971/2159   train_loss = 5.205\n",
      "Epoch   1 Batch  981/2159   train_loss = 5.889\n",
      "Epoch   1 Batch  991/2159   train_loss = 5.558\n",
      "Epoch   1 Batch 1001/2159   train_loss = 5.009\n",
      "Epoch   1 Batch 1011/2159   train_loss = 5.342\n",
      "Epoch   1 Batch 1021/2159   train_loss = 6.387\n",
      "Epoch   1 Batch 1031/2159   train_loss = 5.124\n",
      "Epoch   1 Batch 1041/2159   train_loss = 5.011\n",
      "Epoch   1 Batch 1051/2159   train_loss = 6.010\n",
      "Epoch   1 Batch 1061/2159   train_loss = 5.941\n",
      "Epoch   1 Batch 1071/2159   train_loss = 5.079\n",
      "Epoch   1 Batch 1081/2159   train_loss = 5.914\n",
      "Epoch   1 Batch 1091/2159   train_loss = 5.083\n",
      "Epoch   1 Batch 1101/2159   train_loss = 5.309\n",
      "Epoch   1 Batch 1111/2159   train_loss = 5.499\n",
      "Epoch   1 Batch 1121/2159   train_loss = 5.580\n",
      "Epoch   1 Batch 1131/2159   train_loss = 6.169\n",
      "Epoch   1 Batch 1141/2159   train_loss = 4.775\n",
      "Epoch   1 Batch 1151/2159   train_loss = 5.113\n",
      "Epoch   1 Batch 1161/2159   train_loss = 4.840\n",
      "Epoch   1 Batch 1171/2159   train_loss = 5.384\n",
      "Epoch   1 Batch 1181/2159   train_loss = 6.448\n",
      "Epoch   1 Batch 1191/2159   train_loss = 6.105\n",
      "Epoch   1 Batch 1201/2159   train_loss = 5.105\n",
      "Epoch   1 Batch 1211/2159   train_loss = 5.816\n",
      "Epoch   1 Batch 1221/2159   train_loss = 5.182\n",
      "Epoch   1 Batch 1231/2159   train_loss = 5.005\n",
      "Epoch   1 Batch 1241/2159   train_loss = 6.085\n",
      "Epoch   1 Batch 1251/2159   train_loss = 5.632\n",
      "Epoch   1 Batch 1261/2159   train_loss = 4.060\n",
      "Epoch   1 Batch 1271/2159   train_loss = 5.674\n",
      "Epoch   1 Batch 1281/2159   train_loss = 4.172\n",
      "Epoch   1 Batch 1291/2159   train_loss = 5.963\n",
      "Epoch   1 Batch 1301/2159   train_loss = 4.157\n",
      "Epoch   1 Batch 1311/2159   train_loss = 5.685\n",
      "Epoch   1 Batch 1321/2159   train_loss = 4.675\n",
      "Epoch   1 Batch 1331/2159   train_loss = 5.916\n",
      "Epoch   1 Batch 1341/2159   train_loss = 5.081\n",
      "Epoch   1 Batch 1351/2159   train_loss = 4.713\n",
      "Epoch   1 Batch 1361/2159   train_loss = 5.345\n",
      "Epoch   1 Batch 1371/2159   train_loss = 5.629\n",
      "Epoch   1 Batch 1381/2159   train_loss = 5.744\n",
      "Epoch   1 Batch 1391/2159   train_loss = 4.571\n",
      "Epoch   1 Batch 1401/2159   train_loss = 5.733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1 Batch 1411/2159   train_loss = 4.957\n",
      "Epoch   1 Batch 1421/2159   train_loss = 5.185\n",
      "Epoch   1 Batch 1431/2159   train_loss = 5.599\n",
      "Epoch   1 Batch 1441/2159   train_loss = 4.873\n",
      "Epoch   1 Batch 1451/2159   train_loss = 6.490\n",
      "Epoch   1 Batch 1461/2159   train_loss = 4.607\n",
      "Epoch   1 Batch 1471/2159   train_loss = 5.258\n",
      "Epoch   1 Batch 1481/2159   train_loss = 4.634\n",
      "Epoch   1 Batch 1491/2159   train_loss = 6.220\n",
      "Epoch   1 Batch 1501/2159   train_loss = 4.427\n",
      "Epoch   1 Batch 1511/2159   train_loss = 4.847\n",
      "Epoch   1 Batch 1521/2159   train_loss = 5.785\n",
      "Epoch   1 Batch 1531/2159   train_loss = 6.626\n",
      "Epoch   1 Batch 1541/2159   train_loss = 5.166\n",
      "Epoch   1 Batch 1551/2159   train_loss = 5.487\n",
      "Epoch   1 Batch 1561/2159   train_loss = 4.626\n",
      "Epoch   1 Batch 1571/2159   train_loss = 5.273\n",
      "Epoch   1 Batch 1581/2159   train_loss = 5.285\n",
      "Epoch   1 Batch 1591/2159   train_loss = 5.253\n",
      "Epoch   1 Batch 1601/2159   train_loss = 5.639\n",
      "Epoch   1 Batch 1611/2159   train_loss = 4.464\n",
      "Epoch   1 Batch 1621/2159   train_loss = 5.575\n",
      "Epoch   1 Batch 1631/2159   train_loss = 4.600\n",
      "Epoch   1 Batch 1641/2159   train_loss = 5.372\n",
      "Epoch   1 Batch 1651/2159   train_loss = 6.157\n",
      "Epoch   1 Batch 1661/2159   train_loss = 6.175\n",
      "Epoch   1 Batch 1671/2159   train_loss = 5.395\n",
      "Epoch   1 Batch 1681/2159   train_loss = 4.536\n",
      "Epoch   1 Batch 1691/2159   train_loss = 5.198\n",
      "Epoch   1 Batch 1701/2159   train_loss = 6.294\n",
      "Epoch   1 Batch 1711/2159   train_loss = 6.249\n",
      "Epoch   1 Batch 1721/2159   train_loss = 4.736\n",
      "Epoch   1 Batch 1731/2159   train_loss = 5.955\n",
      "Epoch   1 Batch 1741/2159   train_loss = 5.918\n",
      "Epoch   1 Batch 1751/2159   train_loss = 5.537\n",
      "Epoch   1 Batch 1761/2159   train_loss = 4.990\n",
      "Epoch   1 Batch 1771/2159   train_loss = 5.206\n",
      "Epoch   1 Batch 1781/2159   train_loss = 6.143\n",
      "Epoch   1 Batch 1791/2159   train_loss = 3.881\n",
      "Epoch   1 Batch 1801/2159   train_loss = 5.982\n",
      "Epoch   1 Batch 1811/2159   train_loss = 5.152\n",
      "Epoch   1 Batch 1821/2159   train_loss = 5.082\n",
      "Epoch   1 Batch 1831/2159   train_loss = 5.373\n",
      "Epoch   1 Batch 1841/2159   train_loss = 4.702\n",
      "Epoch   1 Batch 1851/2159   train_loss = 6.610\n",
      "Epoch   1 Batch 1861/2159   train_loss = 4.656\n",
      "Epoch   1 Batch 1871/2159   train_loss = 6.048\n",
      "Epoch   1 Batch 1881/2159   train_loss = 4.319\n",
      "Epoch   1 Batch 1891/2159   train_loss = 5.435\n",
      "Epoch   1 Batch 1901/2159   train_loss = 5.138\n",
      "Epoch   1 Batch 1911/2159   train_loss = 4.595\n",
      "Epoch   1 Batch 1921/2159   train_loss = 5.806\n",
      "Epoch   1 Batch 1931/2159   train_loss = 4.958\n",
      "Epoch   1 Batch 1941/2159   train_loss = 4.919\n",
      "Epoch   1 Batch 1951/2159   train_loss = 4.606\n",
      "Epoch   1 Batch 1961/2159   train_loss = 5.700\n",
      "Epoch   1 Batch 1971/2159   train_loss = 4.942\n",
      "Epoch   1 Batch 1981/2159   train_loss = 7.305\n",
      "Epoch   1 Batch 1991/2159   train_loss = 5.962\n",
      "Epoch   1 Batch 2001/2159   train_loss = 5.642\n",
      "Epoch   1 Batch 2011/2159   train_loss = 3.939\n",
      "Epoch   1 Batch 2021/2159   train_loss = 4.918\n",
      "Epoch   1 Batch 2031/2159   train_loss = 5.812\n",
      "Epoch   1 Batch 2041/2159   train_loss = 4.785\n",
      "Epoch   1 Batch 2051/2159   train_loss = 5.405\n",
      "Epoch   1 Batch 2061/2159   train_loss = 4.189\n",
      "Epoch   1 Batch 2071/2159   train_loss = 5.878\n",
      "Epoch   1 Batch 2081/2159   train_loss = 5.093\n",
      "Epoch   1 Batch 2091/2159   train_loss = 5.521\n",
      "Epoch   1 Batch 2101/2159   train_loss = 5.023\n",
      "Epoch   1 Batch 2111/2159   train_loss = 6.566\n",
      "Epoch   1 Batch 2121/2159   train_loss = 4.896\n",
      "Epoch   1 Batch 2131/2159   train_loss = 6.374\n",
      "Epoch   1 Batch 2141/2159   train_loss = 4.477\n",
      "Epoch   1 Batch 2151/2159   train_loss = 5.095\n",
      "Epoch   2 Batch    2/2159   train_loss = 5.102\n",
      "Epoch   2 Batch   12/2159   train_loss = 4.668\n",
      "Epoch   2 Batch   22/2159   train_loss = 5.646\n",
      "Epoch   2 Batch   32/2159   train_loss = 4.521\n",
      "Epoch   2 Batch   42/2159   train_loss = 5.897\n",
      "Epoch   2 Batch   52/2159   train_loss = 3.921\n",
      "Epoch   2 Batch   62/2159   train_loss = 5.595\n",
      "Epoch   2 Batch   72/2159   train_loss = 5.200\n",
      "Epoch   2 Batch   82/2159   train_loss = 5.573\n",
      "Epoch   2 Batch   92/2159   train_loss = 5.051\n",
      "Epoch   2 Batch  102/2159   train_loss = 4.862\n",
      "Epoch   2 Batch  112/2159   train_loss = 5.152\n",
      "Epoch   2 Batch  122/2159   train_loss = 5.813\n",
      "Epoch   2 Batch  132/2159   train_loss = 4.937\n",
      "Epoch   2 Batch  142/2159   train_loss = 4.167\n",
      "Epoch   2 Batch  152/2159   train_loss = 5.247\n",
      "Epoch   2 Batch  162/2159   train_loss = 5.705\n",
      "Epoch   2 Batch  172/2159   train_loss = 4.609\n",
      "Epoch   2 Batch  182/2159   train_loss = 4.552\n",
      "Epoch   2 Batch  192/2159   train_loss = 4.704\n",
      "Epoch   2 Batch  202/2159   train_loss = 5.741\n",
      "Epoch   2 Batch  212/2159   train_loss = 5.610\n",
      "Epoch   2 Batch  222/2159   train_loss = 4.306\n",
      "Epoch   2 Batch  232/2159   train_loss = 5.089\n",
      "Epoch   2 Batch  242/2159   train_loss = 5.954\n",
      "Epoch   2 Batch  252/2159   train_loss = 4.947\n",
      "Epoch   2 Batch  262/2159   train_loss = 4.797\n",
      "Epoch   2 Batch  272/2159   train_loss = 4.531\n",
      "Epoch   2 Batch  282/2159   train_loss = 4.462\n",
      "Epoch   2 Batch  292/2159   train_loss = 5.355\n",
      "Epoch   2 Batch  302/2159   train_loss = 4.713\n",
      "Epoch   2 Batch  312/2159   train_loss = 5.169\n",
      "Epoch   2 Batch  322/2159   train_loss = 5.143\n",
      "Epoch   2 Batch  332/2159   train_loss = 4.392\n",
      "Epoch   2 Batch  342/2159   train_loss = 5.160\n",
      "Epoch   2 Batch  352/2159   train_loss = 4.429\n",
      "Epoch   2 Batch  362/2159   train_loss = 5.097\n",
      "Epoch   2 Batch  372/2159   train_loss = 4.346\n",
      "Epoch   2 Batch  382/2159   train_loss = 4.014\n",
      "Epoch   2 Batch  392/2159   train_loss = 5.201\n",
      "Epoch   2 Batch  402/2159   train_loss = 4.579\n",
      "Epoch   2 Batch  412/2159   train_loss = 4.815\n",
      "Epoch   2 Batch  422/2159   train_loss = 5.222\n",
      "Epoch   2 Batch  432/2159   train_loss = 4.837\n",
      "Epoch   2 Batch  442/2159   train_loss = 3.577\n",
      "Epoch   2 Batch  452/2159   train_loss = 4.323\n",
      "Epoch   2 Batch  462/2159   train_loss = 5.530\n",
      "Epoch   2 Batch  472/2159   train_loss = 4.629\n",
      "Epoch   2 Batch  482/2159   train_loss = 5.359\n",
      "Epoch   2 Batch  492/2159   train_loss = 5.435\n",
      "Epoch   2 Batch  502/2159   train_loss = 5.818\n",
      "Epoch   2 Batch  512/2159   train_loss = 5.142\n",
      "Epoch   2 Batch  522/2159   train_loss = 5.830\n",
      "Epoch   2 Batch  532/2159   train_loss = 4.239\n",
      "Epoch   2 Batch  542/2159   train_loss = 5.353\n",
      "Epoch   2 Batch  552/2159   train_loss = 5.219\n",
      "Epoch   2 Batch  562/2159   train_loss = 5.021\n",
      "Epoch   2 Batch  572/2159   train_loss = 4.224\n",
      "Epoch   2 Batch  582/2159   train_loss = 5.114\n",
      "Epoch   2 Batch  592/2159   train_loss = 5.100\n",
      "Epoch   2 Batch  602/2159   train_loss = 4.594\n",
      "Epoch   2 Batch  612/2159   train_loss = 6.139\n",
      "Epoch   2 Batch  622/2159   train_loss = 5.387\n",
      "Epoch   2 Batch  632/2159   train_loss = 5.386\n",
      "Epoch   2 Batch  642/2159   train_loss = 4.457\n",
      "Epoch   2 Batch  652/2159   train_loss = 5.412\n",
      "Epoch   2 Batch  662/2159   train_loss = 4.203\n",
      "Epoch   2 Batch  672/2159   train_loss = 4.842\n",
      "Epoch   2 Batch  682/2159   train_loss = 5.174\n",
      "Epoch   2 Batch  692/2159   train_loss = 5.181\n",
      "Epoch   2 Batch  702/2159   train_loss = 4.889\n",
      "Epoch   2 Batch  712/2159   train_loss = 5.068\n",
      "Epoch   2 Batch  722/2159   train_loss = 4.049\n",
      "Epoch   2 Batch  732/2159   train_loss = 4.709\n",
      "Epoch   2 Batch  742/2159   train_loss = 5.726\n",
      "Epoch   2 Batch  752/2159   train_loss = 6.306\n",
      "Epoch   2 Batch  762/2159   train_loss = 5.045\n",
      "Epoch   2 Batch  772/2159   train_loss = 4.792\n",
      "Epoch   2 Batch  782/2159   train_loss = 5.482\n",
      "Epoch   2 Batch  792/2159   train_loss = 5.523\n",
      "Epoch   2 Batch  802/2159   train_loss = 4.546\n",
      "Epoch   2 Batch  812/2159   train_loss = 5.275\n",
      "Epoch   2 Batch  822/2159   train_loss = 5.639\n",
      "Epoch   2 Batch  832/2159   train_loss = 4.747\n",
      "Epoch   2 Batch  842/2159   train_loss = 4.889\n",
      "Epoch   2 Batch  852/2159   train_loss = 5.160\n",
      "Epoch   2 Batch  862/2159   train_loss = 4.702\n",
      "Epoch   2 Batch  872/2159   train_loss = 5.058\n",
      "Epoch   2 Batch  882/2159   train_loss = 4.354\n",
      "Epoch   2 Batch  892/2159   train_loss = 4.375\n",
      "Epoch   2 Batch  902/2159   train_loss = 4.940\n",
      "Epoch   2 Batch  912/2159   train_loss = 5.170\n",
      "Epoch   2 Batch  922/2159   train_loss = 5.858\n",
      "Epoch   2 Batch  932/2159   train_loss = 4.590\n",
      "Epoch   2 Batch  942/2159   train_loss = 4.739\n",
      "Epoch   2 Batch  952/2159   train_loss = 5.768\n",
      "Epoch   2 Batch  962/2159   train_loss = 5.188\n",
      "Epoch   2 Batch  972/2159   train_loss = 5.005\n",
      "Epoch   2 Batch  982/2159   train_loss = 5.216\n",
      "Epoch   2 Batch  992/2159   train_loss = 5.411\n",
      "Epoch   2 Batch 1002/2159   train_loss = 4.145\n",
      "Epoch   2 Batch 1012/2159   train_loss = 5.116\n",
      "Epoch   2 Batch 1022/2159   train_loss = 4.975\n",
      "Epoch   2 Batch 1032/2159   train_loss = 4.691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2 Batch 1042/2159   train_loss = 4.688\n",
      "Epoch   2 Batch 1052/2159   train_loss = 5.929\n",
      "Epoch   2 Batch 1062/2159   train_loss = 4.303\n",
      "Epoch   2 Batch 1072/2159   train_loss = 5.194\n",
      "Epoch   2 Batch 1082/2159   train_loss = 5.690\n",
      "Epoch   2 Batch 1092/2159   train_loss = 4.890\n",
      "Epoch   2 Batch 1102/2159   train_loss = 4.582\n",
      "Epoch   2 Batch 1112/2159   train_loss = 5.446\n",
      "Epoch   2 Batch 1122/2159   train_loss = 5.748\n",
      "Epoch   2 Batch 1132/2159   train_loss = 4.626\n",
      "Epoch   2 Batch 1142/2159   train_loss = 5.072\n",
      "Epoch   2 Batch 1152/2159   train_loss = 4.304\n",
      "Epoch   2 Batch 1162/2159   train_loss = 4.779\n",
      "Epoch   2 Batch 1172/2159   train_loss = 5.434\n",
      "Epoch   2 Batch 1182/2159   train_loss = 6.509\n",
      "Epoch   2 Batch 1192/2159   train_loss = 6.462\n",
      "Epoch   2 Batch 1202/2159   train_loss = 5.181\n",
      "Epoch   2 Batch 1212/2159   train_loss = 5.524\n",
      "Epoch   2 Batch 1222/2159   train_loss = 5.537\n",
      "Epoch   2 Batch 1232/2159   train_loss = 5.913\n",
      "Epoch   2 Batch 1242/2159   train_loss = 4.371\n",
      "Epoch   2 Batch 1252/2159   train_loss = 4.878\n",
      "Epoch   2 Batch 1262/2159   train_loss = 4.319\n",
      "Epoch   2 Batch 1272/2159   train_loss = 4.952\n",
      "Epoch   2 Batch 1282/2159   train_loss = 4.835\n",
      "Epoch   2 Batch 1292/2159   train_loss = 4.931\n",
      "Epoch   2 Batch 1302/2159   train_loss = 5.956\n",
      "Epoch   2 Batch 1312/2159   train_loss = 5.331\n",
      "Epoch   2 Batch 1322/2159   train_loss = 5.664\n",
      "Epoch   2 Batch 1332/2159   train_loss = 5.043\n",
      "Epoch   2 Batch 1342/2159   train_loss = 6.097\n",
      "Epoch   2 Batch 1352/2159   train_loss = 4.463\n",
      "Epoch   2 Batch 1362/2159   train_loss = 5.140\n",
      "Epoch   2 Batch 1372/2159   train_loss = 4.835\n",
      "Epoch   2 Batch 1382/2159   train_loss = 5.327\n",
      "Epoch   2 Batch 1392/2159   train_loss = 4.760\n",
      "Epoch   2 Batch 1402/2159   train_loss = 5.130\n",
      "Epoch   2 Batch 1412/2159   train_loss = 4.725\n",
      "Epoch   2 Batch 1422/2159   train_loss = 5.638\n",
      "Epoch   2 Batch 1432/2159   train_loss = 5.187\n",
      "Epoch   2 Batch 1442/2159   train_loss = 4.760\n",
      "Epoch   2 Batch 1452/2159   train_loss = 5.874\n",
      "Epoch   2 Batch 1462/2159   train_loss = 4.731\n",
      "Epoch   2 Batch 1472/2159   train_loss = 4.577\n",
      "Epoch   2 Batch 1482/2159   train_loss = 4.737\n",
      "Epoch   2 Batch 1492/2159   train_loss = 4.950\n",
      "Epoch   2 Batch 1502/2159   train_loss = 5.457\n",
      "Epoch   2 Batch 1512/2159   train_loss = 4.988\n",
      "Epoch   2 Batch 1522/2159   train_loss = 5.314\n",
      "Epoch   2 Batch 1532/2159   train_loss = 4.558\n",
      "Epoch   2 Batch 1542/2159   train_loss = 4.282\n",
      "Epoch   2 Batch 1552/2159   train_loss = 5.260\n",
      "Epoch   2 Batch 1562/2159   train_loss = 5.502\n",
      "Epoch   2 Batch 1572/2159   train_loss = 5.266\n",
      "Epoch   2 Batch 1582/2159   train_loss = 6.286\n",
      "Epoch   2 Batch 1592/2159   train_loss = 5.315\n",
      "Epoch   2 Batch 1602/2159   train_loss = 4.804\n",
      "Epoch   2 Batch 1612/2159   train_loss = 4.832\n",
      "Epoch   2 Batch 1622/2159   train_loss = 5.143\n",
      "Epoch   2 Batch 1632/2159   train_loss = 4.636\n",
      "Epoch   2 Batch 1642/2159   train_loss = 4.602\n",
      "Epoch   2 Batch 1652/2159   train_loss = 5.582\n",
      "Epoch   2 Batch 1662/2159   train_loss = 4.832\n",
      "Epoch   2 Batch 1672/2159   train_loss = 4.805\n",
      "Epoch   2 Batch 1682/2159   train_loss = 4.700\n",
      "Epoch   2 Batch 1692/2159   train_loss = 5.451\n",
      "Epoch   2 Batch 1702/2159   train_loss = 5.128\n",
      "Epoch   2 Batch 1712/2159   train_loss = 4.883\n",
      "Epoch   2 Batch 1722/2159   train_loss = 5.916\n",
      "Epoch   2 Batch 1732/2159   train_loss = 4.779\n",
      "Epoch   2 Batch 1742/2159   train_loss = 4.185\n",
      "Epoch   2 Batch 1752/2159   train_loss = 5.542\n",
      "Epoch   2 Batch 1762/2159   train_loss = 4.511\n",
      "Epoch   2 Batch 1772/2159   train_loss = 4.351\n",
      "Epoch   2 Batch 1782/2159   train_loss = 4.205\n",
      "Epoch   2 Batch 1792/2159   train_loss = 4.889\n",
      "Epoch   2 Batch 1802/2159   train_loss = 4.835\n",
      "Epoch   2 Batch 1812/2159   train_loss = 4.135\n",
      "Epoch   2 Batch 1822/2159   train_loss = 5.264\n",
      "Epoch   2 Batch 1832/2159   train_loss = 4.835\n",
      "Epoch   2 Batch 1842/2159   train_loss = 5.959\n",
      "Epoch   2 Batch 1852/2159   train_loss = 4.499\n",
      "Epoch   2 Batch 1862/2159   train_loss = 4.917\n",
      "Epoch   2 Batch 1872/2159   train_loss = 4.776\n",
      "Epoch   2 Batch 1882/2159   train_loss = 6.236\n",
      "Epoch   2 Batch 1892/2159   train_loss = 4.860\n",
      "Epoch   2 Batch 1902/2159   train_loss = 5.815\n",
      "Epoch   2 Batch 1912/2159   train_loss = 5.836\n",
      "Epoch   2 Batch 1922/2159   train_loss = 3.612\n",
      "Epoch   2 Batch 1932/2159   train_loss = 5.508\n",
      "Epoch   2 Batch 1942/2159   train_loss = 4.992\n",
      "Epoch   2 Batch 1952/2159   train_loss = 5.575\n",
      "Epoch   2 Batch 1962/2159   train_loss = 5.289\n",
      "Epoch   2 Batch 1972/2159   train_loss = 5.251\n",
      "Epoch   2 Batch 1982/2159   train_loss = 5.316\n",
      "Epoch   2 Batch 1992/2159   train_loss = 4.822\n",
      "Epoch   2 Batch 2002/2159   train_loss = 5.571\n",
      "Epoch   2 Batch 2012/2159   train_loss = 3.768\n",
      "Epoch   2 Batch 2022/2159   train_loss = 5.041\n",
      "Epoch   2 Batch 2032/2159   train_loss = 4.964\n",
      "Epoch   2 Batch 2042/2159   train_loss = 4.212\n",
      "Epoch   2 Batch 2052/2159   train_loss = 4.611\n",
      "Epoch   2 Batch 2062/2159   train_loss = 5.589\n",
      "Epoch   2 Batch 2072/2159   train_loss = 5.416\n",
      "Epoch   2 Batch 2082/2159   train_loss = 4.303\n",
      "Epoch   2 Batch 2092/2159   train_loss = 5.383\n",
      "Epoch   2 Batch 2102/2159   train_loss = 5.600\n",
      "Epoch   2 Batch 2112/2159   train_loss = 5.233\n",
      "Epoch   2 Batch 2122/2159   train_loss = 4.525\n",
      "Epoch   2 Batch 2132/2159   train_loss = 5.108\n",
      "Epoch   2 Batch 2142/2159   train_loss = 4.592\n",
      "Epoch   2 Batch 2152/2159   train_loss = 4.587\n",
      "Epoch   3 Batch    3/2159   train_loss = 5.045\n",
      "Epoch   3 Batch   13/2159   train_loss = 5.728\n",
      "Epoch   3 Batch   23/2159   train_loss = 4.591\n",
      "Epoch   3 Batch   33/2159   train_loss = 4.016\n",
      "Epoch   3 Batch   43/2159   train_loss = 5.440\n",
      "Epoch   3 Batch   53/2159   train_loss = 5.484\n",
      "Epoch   3 Batch   63/2159   train_loss = 4.443\n",
      "Epoch   3 Batch   73/2159   train_loss = 4.681\n",
      "Epoch   3 Batch   83/2159   train_loss = 5.619\n",
      "Epoch   3 Batch   93/2159   train_loss = 4.878\n",
      "Epoch   3 Batch  103/2159   train_loss = 4.562\n",
      "Epoch   3 Batch  113/2159   train_loss = 5.946\n",
      "Epoch   3 Batch  123/2159   train_loss = 6.206\n",
      "Epoch   3 Batch  133/2159   train_loss = 5.324\n",
      "Epoch   3 Batch  143/2159   train_loss = 5.178\n",
      "Epoch   3 Batch  153/2159   train_loss = 5.000\n",
      "Epoch   3 Batch  163/2159   train_loss = 5.500\n",
      "Epoch   3 Batch  173/2159   train_loss = 4.578\n",
      "Epoch   3 Batch  183/2159   train_loss = 4.855\n",
      "Epoch   3 Batch  193/2159   train_loss = 5.522\n",
      "Epoch   3 Batch  203/2159   train_loss = 5.411\n",
      "Epoch   3 Batch  213/2159   train_loss = 4.596\n",
      "Epoch   3 Batch  223/2159   train_loss = 5.306\n",
      "Epoch   3 Batch  233/2159   train_loss = 4.173\n",
      "Epoch   3 Batch  243/2159   train_loss = 5.037\n",
      "Epoch   3 Batch  253/2159   train_loss = 5.329\n",
      "Epoch   3 Batch  263/2159   train_loss = 4.907\n",
      "Epoch   3 Batch  273/2159   train_loss = 5.570\n",
      "Epoch   3 Batch  283/2159   train_loss = 5.662\n",
      "Epoch   3 Batch  293/2159   train_loss = 4.752\n",
      "Epoch   3 Batch  303/2159   train_loss = 4.645\n",
      "Epoch   3 Batch  313/2159   train_loss = 4.744\n",
      "Epoch   3 Batch  323/2159   train_loss = 4.858\n",
      "Epoch   3 Batch  333/2159   train_loss = 4.068\n",
      "Epoch   3 Batch  343/2159   train_loss = 5.109\n",
      "Epoch   3 Batch  353/2159   train_loss = 4.262\n",
      "Epoch   3 Batch  363/2159   train_loss = 4.843\n",
      "Epoch   3 Batch  373/2159   train_loss = 4.678\n",
      "Epoch   3 Batch  383/2159   train_loss = 5.253\n",
      "Epoch   3 Batch  393/2159   train_loss = 5.647\n",
      "Epoch   3 Batch  403/2159   train_loss = 4.972\n",
      "Epoch   3 Batch  413/2159   train_loss = 5.641\n",
      "Epoch   3 Batch  423/2159   train_loss = 4.575\n",
      "Epoch   3 Batch  433/2159   train_loss = 5.217\n",
      "Epoch   3 Batch  443/2159   train_loss = 5.716\n",
      "Epoch   3 Batch  453/2159   train_loss = 4.287\n",
      "Epoch   3 Batch  463/2159   train_loss = 4.657\n",
      "Epoch   3 Batch  473/2159   train_loss = 5.706\n",
      "Epoch   3 Batch  483/2159   train_loss = 5.359\n",
      "Epoch   3 Batch  493/2159   train_loss = 5.147\n",
      "Epoch   3 Batch  503/2159   train_loss = 5.215\n",
      "Epoch   3 Batch  513/2159   train_loss = 4.120\n",
      "Epoch   3 Batch  523/2159   train_loss = 5.487\n",
      "Epoch   3 Batch  533/2159   train_loss = 5.743\n",
      "Epoch   3 Batch  543/2159   train_loss = 4.979\n",
      "Epoch   3 Batch  553/2159   train_loss = 4.497\n",
      "Epoch   3 Batch  563/2159   train_loss = 5.000\n",
      "Epoch   3 Batch  573/2159   train_loss = 4.295\n",
      "Epoch   3 Batch  583/2159   train_loss = 4.303\n",
      "Epoch   3 Batch  593/2159   train_loss = 4.999\n",
      "Epoch   3 Batch  603/2159   train_loss = 5.052\n",
      "Epoch   3 Batch  613/2159   train_loss = 4.587\n",
      "Epoch   3 Batch  623/2159   train_loss = 5.331\n",
      "Epoch   3 Batch  633/2159   train_loss = 5.466\n",
      "Epoch   3 Batch  643/2159   train_loss = 5.453\n",
      "Epoch   3 Batch  653/2159   train_loss = 4.930\n",
      "Epoch   3 Batch  663/2159   train_loss = 5.603\n",
      "Epoch   3 Batch  673/2159   train_loss = 5.762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3 Batch  683/2159   train_loss = 4.672\n",
      "Epoch   3 Batch  693/2159   train_loss = 4.868\n",
      "Epoch   3 Batch  703/2159   train_loss = 5.445\n",
      "Epoch   3 Batch  713/2159   train_loss = 4.606\n",
      "Epoch   3 Batch  723/2159   train_loss = 5.657\n",
      "Epoch   3 Batch  733/2159   train_loss = 4.982\n",
      "Epoch   3 Batch  743/2159   train_loss = 4.611\n",
      "Epoch   3 Batch  753/2159   train_loss = 5.192\n",
      "Epoch   3 Batch  763/2159   train_loss = 5.355\n",
      "Epoch   3 Batch  773/2159   train_loss = 4.671\n",
      "Epoch   3 Batch  783/2159   train_loss = 5.304\n",
      "Epoch   3 Batch  793/2159   train_loss = 4.771\n",
      "Epoch   3 Batch  803/2159   train_loss = 5.078\n",
      "Epoch   3 Batch  813/2159   train_loss = 4.801\n",
      "Epoch   3 Batch  823/2159   train_loss = 5.344\n",
      "Epoch   3 Batch  833/2159   train_loss = 5.622\n",
      "Epoch   3 Batch  843/2159   train_loss = 4.875\n",
      "Epoch   3 Batch  853/2159   train_loss = 4.460\n",
      "Epoch   3 Batch  863/2159   train_loss = 4.061\n",
      "Epoch   3 Batch  873/2159   train_loss = 5.161\n",
      "Epoch   3 Batch  883/2159   train_loss = 4.539\n",
      "Epoch   3 Batch  893/2159   train_loss = 4.381\n",
      "Epoch   3 Batch  903/2159   train_loss = 4.673\n",
      "Epoch   3 Batch  913/2159   train_loss = 5.161\n",
      "Epoch   3 Batch  923/2159   train_loss = 4.669\n",
      "Epoch   3 Batch  933/2159   train_loss = 4.960\n",
      "Epoch   3 Batch  943/2159   train_loss = 5.485\n",
      "Epoch   3 Batch  953/2159   train_loss = 4.130\n",
      "Epoch   3 Batch  963/2159   train_loss = 5.392\n",
      "Epoch   3 Batch  973/2159   train_loss = 5.067\n",
      "Epoch   3 Batch  983/2159   train_loss = 5.396\n",
      "Epoch   3 Batch  993/2159   train_loss = 5.047\n",
      "Epoch   3 Batch 1003/2159   train_loss = 4.380\n",
      "Epoch   3 Batch 1013/2159   train_loss = 4.779\n",
      "Epoch   3 Batch 1023/2159   train_loss = 4.494\n",
      "Epoch   3 Batch 1033/2159   train_loss = 5.114\n",
      "Epoch   3 Batch 1043/2159   train_loss = 4.422\n",
      "Epoch   3 Batch 1053/2159   train_loss = 5.504\n",
      "Epoch   3 Batch 1063/2159   train_loss = 5.893\n",
      "Epoch   3 Batch 1073/2159   train_loss = 5.478\n",
      "Epoch   3 Batch 1083/2159   train_loss = 5.325\n",
      "Epoch   3 Batch 1093/2159   train_loss = 4.263\n",
      "Epoch   3 Batch 1103/2159   train_loss = 4.542\n",
      "Epoch   3 Batch 1113/2159   train_loss = 5.455\n",
      "Epoch   3 Batch 1123/2159   train_loss = 4.384\n",
      "Epoch   3 Batch 1133/2159   train_loss = 5.541\n",
      "Epoch   3 Batch 1143/2159   train_loss = 5.949\n",
      "Epoch   3 Batch 1153/2159   train_loss = 4.965\n",
      "Epoch   3 Batch 1163/2159   train_loss = 4.859\n",
      "Epoch   3 Batch 1173/2159   train_loss = 4.521\n",
      "Epoch   3 Batch 1183/2159   train_loss = 5.062\n",
      "Epoch   3 Batch 1193/2159   train_loss = 3.545\n",
      "Epoch   3 Batch 1203/2159   train_loss = 5.640\n",
      "Epoch   3 Batch 1213/2159   train_loss = 4.774\n",
      "Epoch   3 Batch 1223/2159   train_loss = 3.743\n",
      "Epoch   3 Batch 1233/2159   train_loss = 5.284\n",
      "Epoch   3 Batch 1243/2159   train_loss = 5.119\n",
      "Epoch   3 Batch 1253/2159   train_loss = 5.580\n",
      "Epoch   3 Batch 1263/2159   train_loss = 5.300\n",
      "Epoch   3 Batch 1273/2159   train_loss = 5.294\n",
      "Epoch   3 Batch 1283/2159   train_loss = 4.958\n",
      "Epoch   3 Batch 1293/2159   train_loss = 4.759\n",
      "Epoch   3 Batch 1303/2159   train_loss = 5.259\n",
      "Epoch   3 Batch 1313/2159   train_loss = 4.306\n",
      "Epoch   3 Batch 1323/2159   train_loss = 4.418\n",
      "Epoch   3 Batch 1333/2159   train_loss = 5.726\n",
      "Epoch   3 Batch 1343/2159   train_loss = 4.748\n",
      "Epoch   3 Batch 1353/2159   train_loss = 3.894\n",
      "Epoch   3 Batch 1363/2159   train_loss = 4.881\n",
      "Epoch   3 Batch 1373/2159   train_loss = 4.654\n",
      "Epoch   3 Batch 1383/2159   train_loss = 5.198\n",
      "Epoch   3 Batch 1393/2159   train_loss = 4.367\n",
      "Epoch   3 Batch 1403/2159   train_loss = 3.938\n",
      "Epoch   3 Batch 1413/2159   train_loss = 5.108\n",
      "Epoch   3 Batch 1423/2159   train_loss = 4.254\n",
      "Epoch   3 Batch 1433/2159   train_loss = 4.894\n",
      "Epoch   3 Batch 1443/2159   train_loss = 4.671\n",
      "Epoch   3 Batch 1453/2159   train_loss = 5.163\n",
      "Epoch   3 Batch 1463/2159   train_loss = 4.950\n",
      "Epoch   3 Batch 1473/2159   train_loss = 6.246\n",
      "Epoch   3 Batch 1483/2159   train_loss = 4.854\n",
      "Epoch   3 Batch 1493/2159   train_loss = 5.651\n",
      "Epoch   3 Batch 1503/2159   train_loss = 4.971\n",
      "Epoch   3 Batch 1513/2159   train_loss = 4.676\n",
      "Epoch   3 Batch 1523/2159   train_loss = 4.778\n",
      "Epoch   3 Batch 1533/2159   train_loss = 5.345\n",
      "Epoch   3 Batch 1543/2159   train_loss = 4.543\n",
      "Epoch   3 Batch 1553/2159   train_loss = 5.418\n",
      "Epoch   3 Batch 1563/2159   train_loss = 5.142\n",
      "Epoch   3 Batch 1573/2159   train_loss = 4.645\n",
      "Epoch   3 Batch 1583/2159   train_loss = 4.404\n",
      "Epoch   3 Batch 1593/2159   train_loss = 4.923\n",
      "Epoch   3 Batch 1603/2159   train_loss = 5.292\n",
      "Epoch   3 Batch 1613/2159   train_loss = 5.242\n",
      "Epoch   3 Batch 1623/2159   train_loss = 4.940\n",
      "Epoch   3 Batch 1633/2159   train_loss = 4.571\n",
      "Epoch   3 Batch 1643/2159   train_loss = 4.665\n",
      "Epoch   3 Batch 1653/2159   train_loss = 5.428\n",
      "Epoch   3 Batch 1663/2159   train_loss = 4.644\n",
      "Epoch   3 Batch 1673/2159   train_loss = 4.766\n",
      "Epoch   3 Batch 1683/2159   train_loss = 5.225\n",
      "Epoch   3 Batch 1693/2159   train_loss = 5.111\n",
      "Epoch   3 Batch 1703/2159   train_loss = 5.520\n",
      "Epoch   3 Batch 1713/2159   train_loss = 5.233\n",
      "Epoch   3 Batch 1723/2159   train_loss = 5.852\n",
      "Epoch   3 Batch 1733/2159   train_loss = 4.728\n",
      "Epoch   3 Batch 1743/2159   train_loss = 4.766\n",
      "Epoch   3 Batch 1753/2159   train_loss = 4.744\n",
      "Epoch   3 Batch 1763/2159   train_loss = 4.139\n",
      "Epoch   3 Batch 1773/2159   train_loss = 5.017\n",
      "Epoch   3 Batch 1783/2159   train_loss = 5.026\n",
      "Epoch   3 Batch 1793/2159   train_loss = 4.622\n",
      "Epoch   3 Batch 1803/2159   train_loss = 4.768\n",
      "Epoch   3 Batch 1813/2159   train_loss = 5.022\n",
      "Epoch   3 Batch 1823/2159   train_loss = 5.312\n",
      "Epoch   3 Batch 1833/2159   train_loss = 3.975\n",
      "Epoch   3 Batch 1843/2159   train_loss = 4.946\n",
      "Epoch   3 Batch 1853/2159   train_loss = 4.332\n",
      "Epoch   3 Batch 1863/2159   train_loss = 5.021\n",
      "Epoch   3 Batch 1873/2159   train_loss = 5.370\n",
      "Epoch   3 Batch 1883/2159   train_loss = 4.306\n",
      "Epoch   3 Batch 1893/2159   train_loss = 4.996\n",
      "Epoch   3 Batch 1903/2159   train_loss = 4.439\n",
      "Epoch   3 Batch 1913/2159   train_loss = 4.505\n",
      "Epoch   3 Batch 1923/2159   train_loss = 5.241\n",
      "Epoch   3 Batch 1933/2159   train_loss = 5.131\n",
      "Epoch   3 Batch 1943/2159   train_loss = 4.562\n",
      "Epoch   3 Batch 1953/2159   train_loss = 4.998\n",
      "Epoch   3 Batch 1963/2159   train_loss = 4.664\n",
      "Epoch   3 Batch 1973/2159   train_loss = 5.312\n",
      "Epoch   3 Batch 1983/2159   train_loss = 6.384\n",
      "Epoch   3 Batch 1993/2159   train_loss = 5.801\n",
      "Epoch   3 Batch 2003/2159   train_loss = 5.336\n",
      "Epoch   3 Batch 2013/2159   train_loss = 4.670\n",
      "Epoch   3 Batch 2023/2159   train_loss = 5.147\n",
      "Epoch   3 Batch 2033/2159   train_loss = 4.234\n",
      "Epoch   3 Batch 2043/2159   train_loss = 4.685\n",
      "Epoch   3 Batch 2053/2159   train_loss = 5.253\n",
      "Epoch   3 Batch 2063/2159   train_loss = 3.880\n",
      "Epoch   3 Batch 2073/2159   train_loss = 3.983\n",
      "Epoch   3 Batch 2083/2159   train_loss = 5.474\n",
      "Epoch   3 Batch 2093/2159   train_loss = 5.562\n",
      "Epoch   3 Batch 2103/2159   train_loss = 4.504\n",
      "Epoch   3 Batch 2113/2159   train_loss = 4.523\n",
      "Epoch   3 Batch 2123/2159   train_loss = 5.140\n",
      "Epoch   3 Batch 2133/2159   train_loss = 4.399\n",
      "Epoch   3 Batch 2143/2159   train_loss = 4.967\n",
      "Epoch   3 Batch 2153/2159   train_loss = 4.753\n",
      "Epoch   4 Batch    4/2159   train_loss = 6.057\n",
      "Epoch   4 Batch   14/2159   train_loss = 4.808\n",
      "Epoch   4 Batch   24/2159   train_loss = 5.414\n",
      "Epoch   4 Batch   34/2159   train_loss = 4.678\n",
      "Epoch   4 Batch   44/2159   train_loss = 5.212\n",
      "Epoch   4 Batch   54/2159   train_loss = 4.027\n",
      "Epoch   4 Batch   64/2159   train_loss = 5.296\n",
      "Epoch   4 Batch   74/2159   train_loss = 5.083\n",
      "Epoch   4 Batch   84/2159   train_loss = 5.168\n",
      "Epoch   4 Batch   94/2159   train_loss = 4.409\n",
      "Epoch   4 Batch  104/2159   train_loss = 5.523\n",
      "Epoch   4 Batch  114/2159   train_loss = 5.102\n",
      "Epoch   4 Batch  124/2159   train_loss = 4.358\n",
      "Epoch   4 Batch  134/2159   train_loss = 5.355\n",
      "Epoch   4 Batch  144/2159   train_loss = 5.783\n",
      "Epoch   4 Batch  154/2159   train_loss = 4.176\n",
      "Epoch   4 Batch  164/2159   train_loss = 4.749\n",
      "Epoch   4 Batch  174/2159   train_loss = 4.390\n",
      "Epoch   4 Batch  184/2159   train_loss = 4.482\n",
      "Epoch   4 Batch  194/2159   train_loss = 5.268\n",
      "Epoch   4 Batch  204/2159   train_loss = 5.737\n",
      "Epoch   4 Batch  214/2159   train_loss = 3.350\n",
      "Epoch   4 Batch  224/2159   train_loss = 4.247\n",
      "Epoch   4 Batch  234/2159   train_loss = 4.839\n",
      "Epoch   4 Batch  244/2159   train_loss = 5.252\n",
      "Epoch   4 Batch  254/2159   train_loss = 4.812\n",
      "Epoch   4 Batch  264/2159   train_loss = 4.969\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4 Batch  274/2159   train_loss = 5.058\n",
      "Epoch   4 Batch  284/2159   train_loss = 4.539\n",
      "Epoch   4 Batch  294/2159   train_loss = 5.494\n",
      "Epoch   4 Batch  304/2159   train_loss = 5.278\n",
      "Epoch   4 Batch  314/2159   train_loss = 5.732\n",
      "Epoch   4 Batch  324/2159   train_loss = 4.822\n",
      "Epoch   4 Batch  334/2159   train_loss = 4.827\n",
      "Epoch   4 Batch  344/2159   train_loss = 5.804\n",
      "Epoch   4 Batch  354/2159   train_loss = 5.296\n",
      "Epoch   4 Batch  364/2159   train_loss = 5.572\n",
      "Epoch   4 Batch  374/2159   train_loss = 4.905\n",
      "Epoch   4 Batch  384/2159   train_loss = 5.614\n",
      "Epoch   4 Batch  394/2159   train_loss = 5.774\n",
      "Epoch   4 Batch  404/2159   train_loss = 5.018\n",
      "Epoch   4 Batch  414/2159   train_loss = 4.372\n",
      "Epoch   4 Batch  424/2159   train_loss = 4.784\n",
      "Epoch   4 Batch  434/2159   train_loss = 4.654\n",
      "Epoch   4 Batch  444/2159   train_loss = 5.314\n",
      "Epoch   4 Batch  454/2159   train_loss = 5.709\n",
      "Epoch   4 Batch  464/2159   train_loss = 5.787\n",
      "Epoch   4 Batch  474/2159   train_loss = 5.206\n",
      "Epoch   4 Batch  484/2159   train_loss = 4.228\n",
      "Epoch   4 Batch  494/2159   train_loss = 5.794\n",
      "Epoch   4 Batch  504/2159   train_loss = 4.514\n",
      "Epoch   4 Batch  514/2159   train_loss = 4.575\n",
      "Epoch   4 Batch  524/2159   train_loss = 4.695\n",
      "Epoch   4 Batch  534/2159   train_loss = 4.554\n",
      "Epoch   4 Batch  544/2159   train_loss = 5.234\n",
      "Epoch   4 Batch  554/2159   train_loss = 4.853\n",
      "Epoch   4 Batch  564/2159   train_loss = 3.917\n",
      "Epoch   4 Batch  574/2159   train_loss = 4.136\n",
      "Epoch   4 Batch  584/2159   train_loss = 6.196\n",
      "Epoch   4 Batch  594/2159   train_loss = 5.152\n",
      "Epoch   4 Batch  604/2159   train_loss = 3.477\n",
      "Epoch   4 Batch  614/2159   train_loss = 5.171\n",
      "Epoch   4 Batch  624/2159   train_loss = 4.871\n",
      "Epoch   4 Batch  634/2159   train_loss = 4.289\n",
      "Epoch   4 Batch  644/2159   train_loss = 5.544\n",
      "Epoch   4 Batch  654/2159   train_loss = 5.429\n",
      "Epoch   4 Batch  664/2159   train_loss = 5.606\n",
      "Epoch   4 Batch  674/2159   train_loss = 4.967\n",
      "Epoch   4 Batch  684/2159   train_loss = 5.183\n",
      "Epoch   4 Batch  694/2159   train_loss = 3.722\n",
      "Epoch   4 Batch  704/2159   train_loss = 5.658\n",
      "Epoch   4 Batch  714/2159   train_loss = 5.411\n",
      "Epoch   4 Batch  724/2159   train_loss = 5.017\n",
      "Epoch   4 Batch  734/2159   train_loss = 4.096\n",
      "Epoch   4 Batch  744/2159   train_loss = 5.105\n",
      "Epoch   4 Batch  754/2159   train_loss = 4.940\n",
      "Epoch   4 Batch  764/2159   train_loss = 5.571\n",
      "Epoch   4 Batch  774/2159   train_loss = 5.290\n",
      "Epoch   4 Batch  784/2159   train_loss = 4.905\n",
      "Epoch   4 Batch  794/2159   train_loss = 4.411\n",
      "Epoch   4 Batch  804/2159   train_loss = 4.808\n",
      "Epoch   4 Batch  814/2159   train_loss = 4.527\n",
      "Epoch   4 Batch  824/2159   train_loss = 5.071\n",
      "Epoch   4 Batch  834/2159   train_loss = 4.809\n",
      "Epoch   4 Batch  844/2159   train_loss = 4.211\n",
      "Epoch   4 Batch  854/2159   train_loss = 4.569\n",
      "Epoch   4 Batch  864/2159   train_loss = 4.576\n",
      "Epoch   4 Batch  874/2159   train_loss = 4.832\n",
      "Epoch   4 Batch  884/2159   train_loss = 4.769\n",
      "Epoch   4 Batch  894/2159   train_loss = 4.681\n",
      "Epoch   4 Batch  904/2159   train_loss = 5.092\n",
      "Epoch   4 Batch  914/2159   train_loss = 4.878\n",
      "Epoch   4 Batch  924/2159   train_loss = 4.473\n",
      "Epoch   4 Batch  934/2159   train_loss = 4.321\n",
      "Epoch   4 Batch  944/2159   train_loss = 4.556\n",
      "Epoch   4 Batch  954/2159   train_loss = 5.106\n",
      "Epoch   4 Batch  964/2159   train_loss = 5.067\n",
      "Epoch   4 Batch  974/2159   train_loss = 5.361\n",
      "Epoch   4 Batch  984/2159   train_loss = 5.067\n",
      "Epoch   4 Batch  994/2159   train_loss = 5.591\n",
      "Epoch   4 Batch 1004/2159   train_loss = 5.560\n",
      "Epoch   4 Batch 1014/2159   train_loss = 4.849\n",
      "Epoch   4 Batch 1024/2159   train_loss = 5.175\n",
      "Epoch   4 Batch 1034/2159   train_loss = 5.234\n",
      "Epoch   4 Batch 1044/2159   train_loss = 4.600\n",
      "Epoch   4 Batch 1054/2159   train_loss = 4.797\n",
      "Epoch   4 Batch 1064/2159   train_loss = 5.101\n",
      "Epoch   4 Batch 1074/2159   train_loss = 5.177\n",
      "Epoch   4 Batch 1084/2159   train_loss = 5.097\n",
      "Epoch   4 Batch 1094/2159   train_loss = 4.483\n",
      "Epoch   4 Batch 1104/2159   train_loss = 4.605\n",
      "Epoch   4 Batch 1114/2159   train_loss = 5.650\n",
      "Epoch   4 Batch 1124/2159   train_loss = 4.363\n",
      "Epoch   4 Batch 1134/2159   train_loss = 5.720\n",
      "Epoch   4 Batch 1144/2159   train_loss = 5.290\n",
      "Epoch   4 Batch 1154/2159   train_loss = 5.640\n",
      "Epoch   4 Batch 1164/2159   train_loss = 5.281\n",
      "Epoch   4 Batch 1174/2159   train_loss = 4.539\n",
      "Epoch   4 Batch 1184/2159   train_loss = 4.462\n",
      "Epoch   4 Batch 1194/2159   train_loss = 4.963\n",
      "Epoch   4 Batch 1204/2159   train_loss = 5.047\n",
      "Epoch   4 Batch 1214/2159   train_loss = 5.995\n",
      "Epoch   4 Batch 1224/2159   train_loss = 5.131\n",
      "Epoch   4 Batch 1234/2159   train_loss = 5.041\n",
      "Epoch   4 Batch 1244/2159   train_loss = 4.089\n",
      "Epoch   4 Batch 1254/2159   train_loss = 4.524\n",
      "Epoch   4 Batch 1264/2159   train_loss = 5.176\n",
      "Epoch   4 Batch 1274/2159   train_loss = 5.119\n",
      "Epoch   4 Batch 1284/2159   train_loss = 5.064\n",
      "Epoch   4 Batch 1294/2159   train_loss = 5.105\n",
      "Epoch   4 Batch 1304/2159   train_loss = 5.342\n",
      "Epoch   4 Batch 1314/2159   train_loss = 4.332\n",
      "Epoch   4 Batch 1324/2159   train_loss = 4.547\n",
      "Epoch   4 Batch 1334/2159   train_loss = 4.915\n",
      "Epoch   4 Batch 1344/2159   train_loss = 5.875\n",
      "Epoch   4 Batch 1354/2159   train_loss = 3.970\n",
      "Epoch   4 Batch 1364/2159   train_loss = 5.068\n",
      "Epoch   4 Batch 1374/2159   train_loss = 4.839\n",
      "Epoch   4 Batch 1384/2159   train_loss = 4.654\n",
      "Epoch   4 Batch 1394/2159   train_loss = 5.351\n",
      "Epoch   4 Batch 1404/2159   train_loss = 4.741\n",
      "Epoch   4 Batch 1414/2159   train_loss = 4.415\n",
      "Epoch   4 Batch 1424/2159   train_loss = 5.986\n",
      "Epoch   4 Batch 1434/2159   train_loss = 4.853\n",
      "Epoch   4 Batch 1444/2159   train_loss = 5.518\n",
      "Epoch   4 Batch 1454/2159   train_loss = 4.316\n",
      "Epoch   4 Batch 1464/2159   train_loss = 4.164\n",
      "Epoch   4 Batch 1474/2159   train_loss = 3.577\n",
      "Epoch   4 Batch 1484/2159   train_loss = 4.268\n",
      "Epoch   4 Batch 1494/2159   train_loss = 5.009\n",
      "Epoch   4 Batch 1504/2159   train_loss = 5.050\n",
      "Epoch   4 Batch 1514/2159   train_loss = 4.515\n",
      "Epoch   4 Batch 1524/2159   train_loss = 5.180\n",
      "Epoch   4 Batch 1534/2159   train_loss = 5.084\n",
      "Epoch   4 Batch 1544/2159   train_loss = 5.401\n",
      "Epoch   4 Batch 1554/2159   train_loss = 5.761\n",
      "Epoch   4 Batch 1564/2159   train_loss = 5.874\n",
      "Epoch   4 Batch 1574/2159   train_loss = 4.150\n",
      "Epoch   4 Batch 1584/2159   train_loss = 5.388\n",
      "Epoch   4 Batch 1594/2159   train_loss = 5.433\n",
      "Epoch   4 Batch 1604/2159   train_loss = 3.973\n",
      "Epoch   4 Batch 1614/2159   train_loss = 4.250\n",
      "Epoch   4 Batch 1624/2159   train_loss = 4.422\n",
      "Epoch   4 Batch 1634/2159   train_loss = 5.051\n",
      "Epoch   4 Batch 1644/2159   train_loss = 5.090\n",
      "Epoch   4 Batch 1654/2159   train_loss = 5.010\n",
      "Epoch   4 Batch 1664/2159   train_loss = 4.874\n",
      "Epoch   4 Batch 1674/2159   train_loss = 4.725\n",
      "Epoch   4 Batch 1684/2159   train_loss = 5.074\n",
      "Epoch   4 Batch 1694/2159   train_loss = 5.125\n",
      "Epoch   4 Batch 1704/2159   train_loss = 5.714\n",
      "Epoch   4 Batch 1714/2159   train_loss = 4.988\n",
      "Epoch   4 Batch 1724/2159   train_loss = 4.989\n",
      "Epoch   4 Batch 1734/2159   train_loss = 4.180\n",
      "Epoch   4 Batch 1744/2159   train_loss = 4.046\n",
      "Epoch   4 Batch 1754/2159   train_loss = 4.697\n",
      "Epoch   4 Batch 1764/2159   train_loss = 5.100\n",
      "Epoch   4 Batch 1774/2159   train_loss = 4.626\n",
      "Epoch   4 Batch 1784/2159   train_loss = 4.498\n",
      "Epoch   4 Batch 1794/2159   train_loss = 4.446\n",
      "Epoch   4 Batch 1804/2159   train_loss = 6.082\n",
      "Epoch   4 Batch 1814/2159   train_loss = 4.144\n",
      "Epoch   4 Batch 1824/2159   train_loss = 5.715\n",
      "Epoch   4 Batch 1834/2159   train_loss = 5.157\n",
      "Epoch   4 Batch 1844/2159   train_loss = 4.945\n",
      "Epoch   4 Batch 1854/2159   train_loss = 5.751\n",
      "Epoch   4 Batch 1864/2159   train_loss = 4.315\n",
      "Epoch   4 Batch 1874/2159   train_loss = 5.227\n",
      "Epoch   4 Batch 1884/2159   train_loss = 4.677\n",
      "Epoch   4 Batch 1894/2159   train_loss = 4.737\n",
      "Epoch   4 Batch 1904/2159   train_loss = 5.157\n",
      "Epoch   4 Batch 1914/2159   train_loss = 5.187\n",
      "Epoch   4 Batch 1924/2159   train_loss = 5.191\n",
      "Epoch   4 Batch 1934/2159   train_loss = 5.523\n",
      "Epoch   4 Batch 1944/2159   train_loss = 4.838\n",
      "Epoch   4 Batch 1954/2159   train_loss = 4.665\n",
      "Epoch   4 Batch 1964/2159   train_loss = 4.795\n",
      "Epoch   4 Batch 1974/2159   train_loss = 4.391\n",
      "Epoch   4 Batch 1984/2159   train_loss = 5.489\n",
      "Epoch   4 Batch 1994/2159   train_loss = 5.369\n",
      "Epoch   4 Batch 2004/2159   train_loss = 4.118\n",
      "Epoch   4 Batch 2014/2159   train_loss = 4.688\n",
      "Epoch   4 Batch 2024/2159   train_loss = 3.870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4 Batch 2034/2159   train_loss = 5.303\n",
      "Epoch   4 Batch 2044/2159   train_loss = 5.167\n",
      "Epoch   4 Batch 2054/2159   train_loss = 4.598\n",
      "Epoch   4 Batch 2064/2159   train_loss = 5.628\n",
      "Epoch   4 Batch 2074/2159   train_loss = 4.636\n",
      "Epoch   4 Batch 2084/2159   train_loss = 5.884\n",
      "Epoch   4 Batch 2094/2159   train_loss = 5.572\n",
      "Epoch   4 Batch 2104/2159   train_loss = 4.273\n",
      "Epoch   4 Batch 2114/2159   train_loss = 5.225\n",
      "Epoch   4 Batch 2124/2159   train_loss = 5.162\n",
      "Epoch   4 Batch 2134/2159   train_loss = 5.953\n",
      "Epoch   4 Batch 2144/2159   train_loss = 4.875\n",
      "Epoch   4 Batch 2154/2159   train_loss = 5.671\n",
      "Epoch   5 Batch    5/2159   train_loss = 5.463\n",
      "Epoch   5 Batch   15/2159   train_loss = 4.812\n",
      "Epoch   5 Batch   25/2159   train_loss = 4.699\n",
      "Epoch   5 Batch   35/2159   train_loss = 5.816\n",
      "Epoch   5 Batch   45/2159   train_loss = 5.565\n",
      "Epoch   5 Batch   55/2159   train_loss = 5.744\n",
      "Epoch   5 Batch   65/2159   train_loss = 5.230\n",
      "Epoch   5 Batch   75/2159   train_loss = 4.834\n",
      "Epoch   5 Batch   85/2159   train_loss = 4.410\n",
      "Epoch   5 Batch   95/2159   train_loss = 5.244\n",
      "Epoch   5 Batch  105/2159   train_loss = 4.453\n",
      "Epoch   5 Batch  115/2159   train_loss = 4.527\n",
      "Epoch   5 Batch  125/2159   train_loss = 4.637\n",
      "Epoch   5 Batch  135/2159   train_loss = 4.860\n",
      "Epoch   5 Batch  145/2159   train_loss = 4.583\n",
      "Epoch   5 Batch  155/2159   train_loss = 4.776\n",
      "Epoch   5 Batch  165/2159   train_loss = 5.946\n",
      "Epoch   5 Batch  175/2159   train_loss = 5.064\n",
      "Epoch   5 Batch  185/2159   train_loss = 4.761\n",
      "Epoch   5 Batch  195/2159   train_loss = 4.848\n",
      "Epoch   5 Batch  205/2159   train_loss = 4.377\n",
      "Epoch   5 Batch  215/2159   train_loss = 4.935\n",
      "Epoch   5 Batch  225/2159   train_loss = 4.599\n",
      "Epoch   5 Batch  235/2159   train_loss = 5.459\n",
      "Epoch   5 Batch  245/2159   train_loss = 5.366\n",
      "Epoch   5 Batch  255/2159   train_loss = 5.912\n",
      "Epoch   5 Batch  265/2159   train_loss = 5.103\n",
      "Epoch   5 Batch  275/2159   train_loss = 5.136\n",
      "Epoch   5 Batch  285/2159   train_loss = 4.958\n",
      "Epoch   5 Batch  295/2159   train_loss = 4.929\n",
      "Epoch   5 Batch  305/2159   train_loss = 4.159\n",
      "Epoch   5 Batch  315/2159   train_loss = 5.274\n",
      "Epoch   5 Batch  325/2159   train_loss = 4.987\n",
      "Epoch   5 Batch  335/2159   train_loss = 4.504\n",
      "Epoch   5 Batch  345/2159   train_loss = 4.828\n",
      "Epoch   5 Batch  355/2159   train_loss = 5.359\n",
      "Epoch   5 Batch  365/2159   train_loss = 4.929\n",
      "Epoch   5 Batch  375/2159   train_loss = 4.710\n",
      "Epoch   5 Batch  385/2159   train_loss = 5.898\n",
      "Epoch   5 Batch  395/2159   train_loss = 4.387\n",
      "Epoch   5 Batch  405/2159   train_loss = 5.306\n",
      "Epoch   5 Batch  415/2159   train_loss = 4.727\n",
      "Epoch   5 Batch  425/2159   train_loss = 5.072\n",
      "Epoch   5 Batch  435/2159   train_loss = 5.506\n",
      "Epoch   5 Batch  445/2159   train_loss = 4.391\n",
      "Epoch   5 Batch  455/2159   train_loss = 4.707\n",
      "Epoch   5 Batch  465/2159   train_loss = 4.851\n",
      "Epoch   5 Batch  475/2159   train_loss = 5.267\n",
      "Epoch   5 Batch  485/2159   train_loss = 5.375\n",
      "Epoch   5 Batch  495/2159   train_loss = 4.692\n",
      "Epoch   5 Batch  505/2159   train_loss = 4.955\n",
      "Epoch   5 Batch  515/2159   train_loss = 4.713\n",
      "Epoch   5 Batch  525/2159   train_loss = 4.568\n",
      "Epoch   5 Batch  535/2159   train_loss = 4.447\n",
      "Epoch   5 Batch  545/2159   train_loss = 4.739\n",
      "Epoch   5 Batch  555/2159   train_loss = 4.444\n",
      "Epoch   5 Batch  565/2159   train_loss = 5.145\n",
      "Epoch   5 Batch  575/2159   train_loss = 3.983\n",
      "Epoch   5 Batch  585/2159   train_loss = 4.821\n",
      "Epoch   5 Batch  595/2159   train_loss = 4.857\n",
      "Epoch   5 Batch  605/2159   train_loss = 4.193\n",
      "Epoch   5 Batch  615/2159   train_loss = 4.936\n",
      "Epoch   5 Batch  625/2159   train_loss = 5.247\n",
      "Epoch   5 Batch  635/2159   train_loss = 5.406\n",
      "Epoch   5 Batch  645/2159   train_loss = 5.156\n",
      "Epoch   5 Batch  655/2159   train_loss = 5.051\n",
      "Epoch   5 Batch  665/2159   train_loss = 4.645\n",
      "Epoch   5 Batch  675/2159   train_loss = 4.856\n",
      "Epoch   5 Batch  685/2159   train_loss = 5.260\n",
      "Epoch   5 Batch  695/2159   train_loss = 5.199\n",
      "Epoch   5 Batch  705/2159   train_loss = 4.835\n",
      "Epoch   5 Batch  715/2159   train_loss = 5.024\n",
      "Epoch   5 Batch  725/2159   train_loss = 5.748\n",
      "Epoch   5 Batch  735/2159   train_loss = 6.061\n",
      "Epoch   5 Batch  745/2159   train_loss = 4.340\n",
      "Epoch   5 Batch  755/2159   train_loss = 5.019\n",
      "Epoch   5 Batch  765/2159   train_loss = 5.814\n",
      "Epoch   5 Batch  775/2159   train_loss = 4.154\n",
      "Epoch   5 Batch  785/2159   train_loss = 4.211\n",
      "Epoch   5 Batch  795/2159   train_loss = 5.543\n",
      "Epoch   5 Batch  805/2159   train_loss = 5.072\n",
      "Epoch   5 Batch  815/2159   train_loss = 4.505\n",
      "Epoch   5 Batch  825/2159   train_loss = 4.654\n",
      "Epoch   5 Batch  835/2159   train_loss = 5.412\n",
      "Epoch   5 Batch  845/2159   train_loss = 4.502\n",
      "Epoch   5 Batch  855/2159   train_loss = 4.111\n",
      "Epoch   5 Batch  865/2159   train_loss = 4.799\n",
      "Epoch   5 Batch  875/2159   train_loss = 4.098\n",
      "Epoch   5 Batch  885/2159   train_loss = 4.699\n",
      "Epoch   5 Batch  895/2159   train_loss = 5.634\n",
      "Epoch   5 Batch  905/2159   train_loss = 4.860\n",
      "Epoch   5 Batch  915/2159   train_loss = 5.093\n",
      "Epoch   5 Batch  925/2159   train_loss = 5.989\n",
      "Epoch   5 Batch  935/2159   train_loss = 4.722\n",
      "Epoch   5 Batch  945/2159   train_loss = 6.268\n",
      "Epoch   5 Batch  955/2159   train_loss = 5.122\n",
      "Epoch   5 Batch  965/2159   train_loss = 4.592\n",
      "Epoch   5 Batch  975/2159   train_loss = 4.783\n",
      "Epoch   5 Batch  985/2159   train_loss = 3.913\n",
      "Epoch   5 Batch  995/2159   train_loss = 4.874\n",
      "Epoch   5 Batch 1005/2159   train_loss = 4.646\n",
      "Epoch   5 Batch 1015/2159   train_loss = 4.501\n",
      "Epoch   5 Batch 1025/2159   train_loss = 4.302\n",
      "Epoch   5 Batch 1035/2159   train_loss = 5.571\n",
      "Epoch   5 Batch 1045/2159   train_loss = 4.107\n",
      "Epoch   5 Batch 1055/2159   train_loss = 4.263\n",
      "Epoch   5 Batch 1065/2159   train_loss = 5.636\n",
      "Epoch   5 Batch 1075/2159   train_loss = 5.416\n",
      "Epoch   5 Batch 1085/2159   train_loss = 5.317\n",
      "Epoch   5 Batch 1095/2159   train_loss = 4.799\n",
      "Epoch   5 Batch 1105/2159   train_loss = 4.838\n",
      "Epoch   5 Batch 1115/2159   train_loss = 3.655\n",
      "Epoch   5 Batch 1125/2159   train_loss = 4.974\n",
      "Epoch   5 Batch 1135/2159   train_loss = 4.357\n",
      "Epoch   5 Batch 1145/2159   train_loss = 4.869\n",
      "Epoch   5 Batch 1155/2159   train_loss = 4.263\n",
      "Epoch   5 Batch 1165/2159   train_loss = 5.068\n",
      "Epoch   5 Batch 1175/2159   train_loss = 4.673\n",
      "Epoch   5 Batch 1185/2159   train_loss = 4.203\n",
      "Epoch   5 Batch 1195/2159   train_loss = 4.602\n",
      "Epoch   5 Batch 1205/2159   train_loss = 5.080\n",
      "Epoch   5 Batch 1215/2159   train_loss = 4.608\n",
      "Epoch   5 Batch 1225/2159   train_loss = 5.032\n",
      "Epoch   5 Batch 1235/2159   train_loss = 5.682\n",
      "Epoch   5 Batch 1245/2159   train_loss = 4.536\n",
      "Epoch   5 Batch 1255/2159   train_loss = 4.549\n",
      "Epoch   5 Batch 1265/2159   train_loss = 4.787\n",
      "Epoch   5 Batch 1275/2159   train_loss = 5.533\n",
      "Epoch   5 Batch 1285/2159   train_loss = 4.456\n",
      "Epoch   5 Batch 1295/2159   train_loss = 5.056\n",
      "Epoch   5 Batch 1305/2159   train_loss = 4.903\n",
      "Epoch   5 Batch 1315/2159   train_loss = 4.849\n",
      "Epoch   5 Batch 1325/2159   train_loss = 4.662\n",
      "Epoch   5 Batch 1335/2159   train_loss = 5.373\n",
      "Epoch   5 Batch 1345/2159   train_loss = 4.739\n",
      "Epoch   5 Batch 1355/2159   train_loss = 4.756\n",
      "Epoch   5 Batch 1365/2159   train_loss = 4.811\n",
      "Epoch   5 Batch 1375/2159   train_loss = 5.676\n",
      "Epoch   5 Batch 1385/2159   train_loss = 5.372\n",
      "Epoch   5 Batch 1395/2159   train_loss = 4.086\n",
      "Epoch   5 Batch 1405/2159   train_loss = 4.934\n",
      "Epoch   5 Batch 1415/2159   train_loss = 5.077\n",
      "Epoch   5 Batch 1425/2159   train_loss = 4.225\n",
      "Epoch   5 Batch 1435/2159   train_loss = 4.350\n",
      "Epoch   5 Batch 1445/2159   train_loss = 4.514\n",
      "Epoch   5 Batch 1455/2159   train_loss = 4.358\n",
      "Epoch   5 Batch 1465/2159   train_loss = 5.247\n",
      "Epoch   5 Batch 1475/2159   train_loss = 5.213\n",
      "Epoch   5 Batch 1485/2159   train_loss = 4.544\n",
      "Epoch   5 Batch 1495/2159   train_loss = 4.592\n",
      "Epoch   5 Batch 1505/2159   train_loss = 4.539\n",
      "Epoch   5 Batch 1515/2159   train_loss = 4.558\n",
      "Epoch   5 Batch 1525/2159   train_loss = 5.374\n",
      "Epoch   5 Batch 1535/2159   train_loss = 4.767\n",
      "Epoch   5 Batch 1545/2159   train_loss = 5.291\n",
      "Epoch   5 Batch 1555/2159   train_loss = 5.143\n",
      "Epoch   5 Batch 1565/2159   train_loss = 4.800\n",
      "Epoch   5 Batch 1575/2159   train_loss = 5.012\n",
      "Epoch   5 Batch 1585/2159   train_loss = 4.690\n",
      "Epoch   5 Batch 1595/2159   train_loss = 5.252\n",
      "Epoch   5 Batch 1605/2159   train_loss = 5.560\n",
      "Epoch   5 Batch 1615/2159   train_loss = 5.247\n",
      "Epoch   5 Batch 1625/2159   train_loss = 5.086\n",
      "Epoch   5 Batch 1635/2159   train_loss = 4.892\n",
      "Epoch   5 Batch 1645/2159   train_loss = 5.156\n",
      "Epoch   5 Batch 1655/2159   train_loss = 4.833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5 Batch 1665/2159   train_loss = 5.066\n",
      "Epoch   5 Batch 1675/2159   train_loss = 5.213\n",
      "Epoch   5 Batch 1685/2159   train_loss = 4.903\n",
      "Epoch   5 Batch 1695/2159   train_loss = 3.853\n",
      "Epoch   5 Batch 1705/2159   train_loss = 5.554\n",
      "Epoch   5 Batch 1715/2159   train_loss = 5.081\n",
      "Epoch   5 Batch 1725/2159   train_loss = 4.456\n",
      "Epoch   5 Batch 1735/2159   train_loss = 4.856\n",
      "Epoch   5 Batch 1745/2159   train_loss = 5.054\n",
      "Epoch   5 Batch 1755/2159   train_loss = 4.513\n",
      "Epoch   5 Batch 1765/2159   train_loss = 4.081\n",
      "Epoch   5 Batch 1775/2159   train_loss = 4.995\n",
      "Epoch   5 Batch 1785/2159   train_loss = 4.654\n",
      "Epoch   5 Batch 1795/2159   train_loss = 4.129\n",
      "Epoch   5 Batch 1805/2159   train_loss = 4.251\n",
      "Epoch   5 Batch 1815/2159   train_loss = 4.604\n",
      "Epoch   5 Batch 1825/2159   train_loss = 5.608\n",
      "Epoch   5 Batch 1835/2159   train_loss = 4.944\n",
      "Epoch   5 Batch 1845/2159   train_loss = 4.488\n",
      "Epoch   5 Batch 1855/2159   train_loss = 4.507\n",
      "Epoch   5 Batch 1865/2159   train_loss = 4.290\n",
      "Epoch   5 Batch 1875/2159   train_loss = 5.052\n",
      "Epoch   5 Batch 1885/2159   train_loss = 5.688\n",
      "Epoch   5 Batch 1895/2159   train_loss = 5.572\n",
      "Epoch   5 Batch 1905/2159   train_loss = 4.837\n",
      "Epoch   5 Batch 1915/2159   train_loss = 4.915\n",
      "Epoch   5 Batch 1925/2159   train_loss = 4.901\n",
      "Epoch   5 Batch 1935/2159   train_loss = 5.584\n",
      "Epoch   5 Batch 1945/2159   train_loss = 5.557\n",
      "Epoch   5 Batch 1955/2159   train_loss = 5.608\n",
      "Epoch   5 Batch 1965/2159   train_loss = 5.675\n",
      "Epoch   5 Batch 1975/2159   train_loss = 4.791\n",
      "Epoch   5 Batch 1985/2159   train_loss = 5.139\n",
      "Epoch   5 Batch 1995/2159   train_loss = 4.768\n",
      "Epoch   5 Batch 2005/2159   train_loss = 5.253\n",
      "Epoch   5 Batch 2015/2159   train_loss = 4.903\n",
      "Epoch   5 Batch 2025/2159   train_loss = 4.525\n",
      "Epoch   5 Batch 2035/2159   train_loss = 4.556\n",
      "Epoch   5 Batch 2045/2159   train_loss = 4.097\n",
      "Epoch   5 Batch 2055/2159   train_loss = 4.738\n",
      "Epoch   5 Batch 2065/2159   train_loss = 3.946\n",
      "Epoch   5 Batch 2075/2159   train_loss = 5.938\n",
      "Epoch   5 Batch 2085/2159   train_loss = 5.223\n",
      "Epoch   5 Batch 2095/2159   train_loss = 4.167\n",
      "Epoch   5 Batch 2105/2159   train_loss = 4.762\n",
      "Epoch   5 Batch 2115/2159   train_loss = 4.952\n",
      "Epoch   5 Batch 2125/2159   train_loss = 3.851\n",
      "Epoch   5 Batch 2135/2159   train_loss = 4.913\n",
      "Epoch   5 Batch 2145/2159   train_loss = 5.365\n",
      "Epoch   5 Batch 2155/2159   train_loss = 4.617\n",
      "Epoch   6 Batch    6/2159   train_loss = 4.235\n",
      "Epoch   6 Batch   16/2159   train_loss = 5.521\n",
      "Epoch   6 Batch   26/2159   train_loss = 5.747\n",
      "Epoch   6 Batch   36/2159   train_loss = 4.450\n",
      "Epoch   6 Batch   46/2159   train_loss = 4.515\n",
      "Epoch   6 Batch   56/2159   train_loss = 4.793\n",
      "Epoch   6 Batch   66/2159   train_loss = 5.128\n",
      "Epoch   6 Batch   76/2159   train_loss = 5.920\n",
      "Epoch   6 Batch   86/2159   train_loss = 5.275\n",
      "Epoch   6 Batch   96/2159   train_loss = 4.695\n",
      "Epoch   6 Batch  106/2159   train_loss = 4.652\n",
      "Epoch   6 Batch  116/2159   train_loss = 4.554\n",
      "Epoch   6 Batch  126/2159   train_loss = 2.967\n",
      "Epoch   6 Batch  136/2159   train_loss = 5.777\n",
      "Epoch   6 Batch  146/2159   train_loss = 4.192\n",
      "Epoch   6 Batch  156/2159   train_loss = 4.700\n",
      "Epoch   6 Batch  166/2159   train_loss = 4.074\n",
      "Epoch   6 Batch  176/2159   train_loss = 4.377\n",
      "Epoch   6 Batch  186/2159   train_loss = 4.814\n",
      "Epoch   6 Batch  196/2159   train_loss = 4.799\n",
      "Epoch   6 Batch  206/2159   train_loss = 4.228\n",
      "Epoch   6 Batch  216/2159   train_loss = 6.217\n",
      "Epoch   6 Batch  226/2159   train_loss = 4.185\n",
      "Epoch   6 Batch  236/2159   train_loss = 5.325\n",
      "Epoch   6 Batch  246/2159   train_loss = 3.772\n",
      "Epoch   6 Batch  256/2159   train_loss = 4.470\n",
      "Epoch   6 Batch  266/2159   train_loss = 4.436\n",
      "Epoch   6 Batch  276/2159   train_loss = 3.602\n",
      "Epoch   6 Batch  286/2159   train_loss = 4.773\n",
      "Epoch   6 Batch  296/2159   train_loss = 5.063\n",
      "Epoch   6 Batch  306/2159   train_loss = 5.094\n",
      "Epoch   6 Batch  316/2159   train_loss = 4.653\n",
      "Epoch   6 Batch  326/2159   train_loss = 5.230\n",
      "Epoch   6 Batch  336/2159   train_loss = 5.069\n",
      "Epoch   6 Batch  346/2159   train_loss = 4.691\n",
      "Epoch   6 Batch  356/2159   train_loss = 4.093\n",
      "Epoch   6 Batch  366/2159   train_loss = 4.853\n",
      "Epoch   6 Batch  376/2159   train_loss = 4.736\n",
      "Epoch   6 Batch  386/2159   train_loss = 4.559\n",
      "Epoch   6 Batch  396/2159   train_loss = 5.093\n",
      "Epoch   6 Batch  406/2159   train_loss = 5.731\n",
      "Epoch   6 Batch  416/2159   train_loss = 4.557\n",
      "Epoch   6 Batch  426/2159   train_loss = 5.076\n",
      "Epoch   6 Batch  436/2159   train_loss = 5.380\n",
      "Epoch   6 Batch  446/2159   train_loss = 4.831\n",
      "Epoch   6 Batch  456/2159   train_loss = 4.241\n",
      "Epoch   6 Batch  466/2159   train_loss = 3.715\n",
      "Epoch   6 Batch  476/2159   train_loss = 4.533\n",
      "Epoch   6 Batch  486/2159   train_loss = 4.922\n",
      "Epoch   6 Batch  496/2159   train_loss = 4.142\n",
      "Epoch   6 Batch  506/2159   train_loss = 4.293\n",
      "Epoch   6 Batch  516/2159   train_loss = 5.359\n",
      "Epoch   6 Batch  526/2159   train_loss = 5.582\n",
      "Epoch   6 Batch  536/2159   train_loss = 4.629\n",
      "Epoch   6 Batch  546/2159   train_loss = 4.244\n",
      "Epoch   6 Batch  556/2159   train_loss = 4.981\n",
      "Epoch   6 Batch  566/2159   train_loss = 5.154\n",
      "Epoch   6 Batch  576/2159   train_loss = 5.595\n",
      "Epoch   6 Batch  586/2159   train_loss = 5.358\n",
      "Epoch   6 Batch  596/2159   train_loss = 4.814\n",
      "Epoch   6 Batch  606/2159   train_loss = 5.369\n",
      "Epoch   6 Batch  616/2159   train_loss = 4.796\n",
      "Epoch   6 Batch  626/2159   train_loss = 6.083\n",
      "Epoch   6 Batch  636/2159   train_loss = 5.563\n",
      "Epoch   6 Batch  646/2159   train_loss = 5.330\n",
      "Epoch   6 Batch  656/2159   train_loss = 4.076\n",
      "Epoch   6 Batch  666/2159   train_loss = 4.660\n",
      "Epoch   6 Batch  676/2159   train_loss = 3.364\n",
      "Epoch   6 Batch  686/2159   train_loss = 4.915\n",
      "Epoch   6 Batch  696/2159   train_loss = 3.796\n",
      "Epoch   6 Batch  706/2159   train_loss = 4.480\n",
      "Epoch   6 Batch  716/2159   train_loss = 4.403\n",
      "Epoch   6 Batch  726/2159   train_loss = 4.886\n",
      "Epoch   6 Batch  736/2159   train_loss = 4.686\n",
      "Epoch   6 Batch  746/2159   train_loss = 4.257\n",
      "Epoch   6 Batch  756/2159   train_loss = 5.705\n",
      "Epoch   6 Batch  766/2159   train_loss = 5.029\n",
      "Epoch   6 Batch  776/2159   train_loss = 4.290\n",
      "Epoch   6 Batch  786/2159   train_loss = 4.271\n",
      "Epoch   6 Batch  796/2159   train_loss = 5.299\n",
      "Epoch   6 Batch  806/2159   train_loss = 4.529\n",
      "Epoch   6 Batch  816/2159   train_loss = 5.166\n",
      "Epoch   6 Batch  826/2159   train_loss = 5.024\n",
      "Epoch   6 Batch  836/2159   train_loss = 4.948\n",
      "Epoch   6 Batch  846/2159   train_loss = 4.520\n",
      "Epoch   6 Batch  856/2159   train_loss = 5.242\n",
      "Epoch   6 Batch  866/2159   train_loss = 5.618\n",
      "Epoch   6 Batch  876/2159   train_loss = 5.088\n",
      "Epoch   6 Batch  886/2159   train_loss = 5.093\n",
      "Epoch   6 Batch  896/2159   train_loss = 5.076\n",
      "Epoch   6 Batch  906/2159   train_loss = 4.528\n",
      "Epoch   6 Batch  916/2159   train_loss = 4.382\n",
      "Epoch   6 Batch  926/2159   train_loss = 5.548\n",
      "Epoch   6 Batch  936/2159   train_loss = 5.581\n",
      "Epoch   6 Batch  946/2159   train_loss = 4.868\n",
      "Epoch   6 Batch  956/2159   train_loss = 4.294\n",
      "Epoch   6 Batch  966/2159   train_loss = 5.025\n",
      "Epoch   6 Batch  976/2159   train_loss = 5.357\n",
      "Epoch   6 Batch  986/2159   train_loss = 4.505\n",
      "Epoch   6 Batch  996/2159   train_loss = 5.638\n",
      "Epoch   6 Batch 1006/2159   train_loss = 5.029\n",
      "Epoch   6 Batch 1016/2159   train_loss = 4.610\n",
      "Epoch   6 Batch 1026/2159   train_loss = 4.855\n",
      "Epoch   6 Batch 1036/2159   train_loss = 6.396\n",
      "Epoch   6 Batch 1046/2159   train_loss = 4.904\n",
      "Epoch   6 Batch 1056/2159   train_loss = 4.456\n",
      "Epoch   6 Batch 1066/2159   train_loss = 4.894\n",
      "Epoch   6 Batch 1076/2159   train_loss = 4.079\n",
      "Epoch   6 Batch 1086/2159   train_loss = 4.354\n",
      "Epoch   6 Batch 1096/2159   train_loss = 5.124\n",
      "Epoch   6 Batch 1106/2159   train_loss = 4.317\n",
      "Epoch   6 Batch 1116/2159   train_loss = 5.607\n",
      "Epoch   6 Batch 1126/2159   train_loss = 4.517\n",
      "Epoch   6 Batch 1136/2159   train_loss = 6.378\n",
      "Epoch   6 Batch 1146/2159   train_loss = 5.747\n",
      "Epoch   6 Batch 1156/2159   train_loss = 5.519\n",
      "Epoch   6 Batch 1166/2159   train_loss = 5.879\n",
      "Epoch   6 Batch 1176/2159   train_loss = 5.172\n",
      "Epoch   6 Batch 1186/2159   train_loss = 4.734\n",
      "Epoch   6 Batch 1196/2159   train_loss = 4.726\n",
      "Epoch   6 Batch 1206/2159   train_loss = 5.940\n",
      "Epoch   6 Batch 1216/2159   train_loss = 4.205\n",
      "Epoch   6 Batch 1226/2159   train_loss = 4.349\n",
      "Epoch   6 Batch 1236/2159   train_loss = 4.529\n",
      "Epoch   6 Batch 1246/2159   train_loss = 5.050\n",
      "Epoch   6 Batch 1256/2159   train_loss = 4.622\n",
      "Epoch   6 Batch 1266/2159   train_loss = 4.921\n",
      "Epoch   6 Batch 1276/2159   train_loss = 5.544\n",
      "Epoch   6 Batch 1286/2159   train_loss = 5.690\n",
      "Epoch   6 Batch 1296/2159   train_loss = 4.581\n",
      "Epoch   6 Batch 1306/2159   train_loss = 6.476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   6 Batch 1316/2159   train_loss = 5.061\n",
      "Epoch   6 Batch 1326/2159   train_loss = 5.379\n",
      "Epoch   6 Batch 1336/2159   train_loss = 5.021\n",
      "Epoch   6 Batch 1346/2159   train_loss = 3.643\n",
      "Epoch   6 Batch 1356/2159   train_loss = 4.915\n",
      "Epoch   6 Batch 1366/2159   train_loss = 5.599\n",
      "Epoch   6 Batch 1376/2159   train_loss = 5.784\n",
      "Epoch   6 Batch 1386/2159   train_loss = 4.371\n",
      "Epoch   6 Batch 1396/2159   train_loss = 5.172\n",
      "Epoch   6 Batch 1406/2159   train_loss = 4.632\n",
      "Epoch   6 Batch 1416/2159   train_loss = 4.846\n",
      "Epoch   6 Batch 1426/2159   train_loss = 5.294\n",
      "Epoch   6 Batch 1436/2159   train_loss = 4.526\n",
      "Epoch   6 Batch 1446/2159   train_loss = 3.971\n",
      "Epoch   6 Batch 1456/2159   train_loss = 4.758\n",
      "Epoch   6 Batch 1466/2159   train_loss = 4.413\n",
      "Epoch   6 Batch 1476/2159   train_loss = 4.743\n",
      "Epoch   6 Batch 1486/2159   train_loss = 4.640\n",
      "Epoch   6 Batch 1496/2159   train_loss = 4.699\n",
      "Epoch   6 Batch 1506/2159   train_loss = 5.380\n",
      "Epoch   6 Batch 1516/2159   train_loss = 4.801\n",
      "Epoch   6 Batch 1526/2159   train_loss = 5.173\n",
      "Epoch   6 Batch 1536/2159   train_loss = 5.210\n",
      "Epoch   6 Batch 1546/2159   train_loss = 5.146\n",
      "Epoch   6 Batch 1556/2159   train_loss = 4.610\n",
      "Epoch   6 Batch 1566/2159   train_loss = 5.197\n",
      "Epoch   6 Batch 1576/2159   train_loss = 4.460\n",
      "Epoch   6 Batch 1586/2159   train_loss = 4.498\n",
      "Epoch   6 Batch 1596/2159   train_loss = 4.486\n",
      "Epoch   6 Batch 1606/2159   train_loss = 5.439\n",
      "Epoch   6 Batch 1616/2159   train_loss = 4.282\n",
      "Epoch   6 Batch 1626/2159   train_loss = 6.220\n",
      "Epoch   6 Batch 1636/2159   train_loss = 4.820\n",
      "Epoch   6 Batch 1646/2159   train_loss = 5.842\n",
      "Epoch   6 Batch 1656/2159   train_loss = 4.605\n",
      "Epoch   6 Batch 1666/2159   train_loss = 4.737\n",
      "Epoch   6 Batch 1676/2159   train_loss = 4.447\n",
      "Epoch   6 Batch 1686/2159   train_loss = 4.570\n",
      "Epoch   6 Batch 1696/2159   train_loss = 5.060\n",
      "Epoch   6 Batch 1706/2159   train_loss = 4.337\n",
      "Epoch   6 Batch 1716/2159   train_loss = 4.916\n",
      "Epoch   6 Batch 1726/2159   train_loss = 5.818\n",
      "Epoch   6 Batch 1736/2159   train_loss = 4.837\n",
      "Epoch   6 Batch 1746/2159   train_loss = 3.498\n",
      "Epoch   6 Batch 1756/2159   train_loss = 4.962\n",
      "Epoch   6 Batch 1766/2159   train_loss = 4.858\n",
      "Epoch   6 Batch 1776/2159   train_loss = 4.196\n",
      "Epoch   6 Batch 1786/2159   train_loss = 4.176\n",
      "Epoch   6 Batch 1796/2159   train_loss = 4.622\n",
      "Epoch   6 Batch 1806/2159   train_loss = 4.320\n",
      "Epoch   6 Batch 1816/2159   train_loss = 4.218\n",
      "Epoch   6 Batch 1826/2159   train_loss = 3.861\n",
      "Epoch   6 Batch 1836/2159   train_loss = 4.251\n",
      "Epoch   6 Batch 1846/2159   train_loss = 4.389\n",
      "Epoch   6 Batch 1856/2159   train_loss = 4.752\n",
      "Epoch   6 Batch 1866/2159   train_loss = 3.921\n",
      "Epoch   6 Batch 1876/2159   train_loss = 5.263\n",
      "Epoch   6 Batch 1886/2159   train_loss = 5.071\n",
      "Epoch   6 Batch 1896/2159   train_loss = 4.802\n",
      "Epoch   6 Batch 1906/2159   train_loss = 4.369\n",
      "Epoch   6 Batch 1916/2159   train_loss = 5.006\n",
      "Epoch   6 Batch 1926/2159   train_loss = 5.131\n",
      "Epoch   6 Batch 1936/2159   train_loss = 4.793\n",
      "Epoch   6 Batch 1946/2159   train_loss = 4.438\n",
      "Epoch   6 Batch 1956/2159   train_loss = 4.876\n",
      "Epoch   6 Batch 1966/2159   train_loss = 5.592\n",
      "Epoch   6 Batch 1976/2159   train_loss = 5.415\n",
      "Epoch   6 Batch 1986/2159   train_loss = 5.313\n",
      "Epoch   6 Batch 1996/2159   train_loss = 4.686\n",
      "Epoch   6 Batch 2006/2159   train_loss = 4.525\n",
      "Epoch   6 Batch 2016/2159   train_loss = 5.584\n",
      "Epoch   6 Batch 2026/2159   train_loss = 4.474\n",
      "Epoch   6 Batch 2036/2159   train_loss = 5.956\n",
      "Epoch   6 Batch 2046/2159   train_loss = 5.519\n",
      "Epoch   6 Batch 2056/2159   train_loss = 4.528\n",
      "Epoch   6 Batch 2066/2159   train_loss = 5.468\n",
      "Epoch   6 Batch 2076/2159   train_loss = 3.752\n",
      "Epoch   6 Batch 2086/2159   train_loss = 4.599\n",
      "Epoch   6 Batch 2096/2159   train_loss = 4.785\n",
      "Epoch   6 Batch 2106/2159   train_loss = 4.656\n",
      "Epoch   6 Batch 2116/2159   train_loss = 4.341\n",
      "Epoch   6 Batch 2126/2159   train_loss = 5.659\n",
      "Epoch   6 Batch 2136/2159   train_loss = 4.443\n",
      "Epoch   6 Batch 2146/2159   train_loss = 5.984\n",
      "Epoch   6 Batch 2156/2159   train_loss = 4.707\n",
      "Epoch   7 Batch    7/2159   train_loss = 3.663\n",
      "Epoch   7 Batch   17/2159   train_loss = 4.619\n",
      "Epoch   7 Batch   27/2159   train_loss = 4.712\n",
      "Epoch   7 Batch   37/2159   train_loss = 4.790\n",
      "Epoch   7 Batch   47/2159   train_loss = 5.158\n",
      "Epoch   7 Batch   57/2159   train_loss = 5.476\n",
      "Epoch   7 Batch   67/2159   train_loss = 5.050\n",
      "Epoch   7 Batch   77/2159   train_loss = 5.478\n",
      "Epoch   7 Batch   87/2159   train_loss = 4.512\n",
      "Epoch   7 Batch   97/2159   train_loss = 4.998\n",
      "Epoch   7 Batch  107/2159   train_loss = 4.390\n",
      "Epoch   7 Batch  117/2159   train_loss = 4.943\n",
      "Epoch   7 Batch  127/2159   train_loss = 4.992\n",
      "Epoch   7 Batch  137/2159   train_loss = 4.730\n",
      "Epoch   7 Batch  147/2159   train_loss = 5.172\n",
      "Epoch   7 Batch  157/2159   train_loss = 4.548\n",
      "Epoch   7 Batch  167/2159   train_loss = 4.625\n",
      "Epoch   7 Batch  177/2159   train_loss = 5.493\n",
      "Epoch   7 Batch  187/2159   train_loss = 4.739\n",
      "Epoch   7 Batch  197/2159   train_loss = 5.977\n",
      "Epoch   7 Batch  207/2159   train_loss = 5.047\n",
      "Epoch   7 Batch  217/2159   train_loss = 4.871\n",
      "Epoch   7 Batch  227/2159   train_loss = 5.782\n",
      "Epoch   7 Batch  237/2159   train_loss = 4.461\n",
      "Epoch   7 Batch  247/2159   train_loss = 4.748\n",
      "Epoch   7 Batch  257/2159   train_loss = 4.853\n",
      "Epoch   7 Batch  267/2159   train_loss = 4.737\n",
      "Epoch   7 Batch  277/2159   train_loss = 5.233\n",
      "Epoch   7 Batch  287/2159   train_loss = 4.677\n",
      "Epoch   7 Batch  297/2159   train_loss = 5.165\n",
      "Epoch   7 Batch  307/2159   train_loss = 4.133\n",
      "Epoch   7 Batch  317/2159   train_loss = 5.782\n",
      "Epoch   7 Batch  327/2159   train_loss = 5.307\n",
      "Epoch   7 Batch  337/2159   train_loss = 5.224\n",
      "Epoch   7 Batch  347/2159   train_loss = 4.453\n",
      "Epoch   7 Batch  357/2159   train_loss = 4.373\n",
      "Epoch   7 Batch  367/2159   train_loss = 4.494\n",
      "Epoch   7 Batch  377/2159   train_loss = 5.212\n",
      "Epoch   7 Batch  387/2159   train_loss = 4.940\n",
      "Epoch   7 Batch  397/2159   train_loss = 4.126\n",
      "Epoch   7 Batch  407/2159   train_loss = 5.075\n",
      "Epoch   7 Batch  417/2159   train_loss = 5.258\n",
      "Epoch   7 Batch  427/2159   train_loss = 4.295\n",
      "Epoch   7 Batch  437/2159   train_loss = 4.752\n",
      "Epoch   7 Batch  447/2159   train_loss = 4.625\n",
      "Epoch   7 Batch  457/2159   train_loss = 4.468\n",
      "Epoch   7 Batch  467/2159   train_loss = 5.288\n",
      "Epoch   7 Batch  477/2159   train_loss = 5.273\n",
      "Epoch   7 Batch  487/2159   train_loss = 4.401\n",
      "Epoch   7 Batch  497/2159   train_loss = 3.895\n",
      "Epoch   7 Batch  507/2159   train_loss = 5.338\n",
      "Epoch   7 Batch  517/2159   train_loss = 4.064\n",
      "Epoch   7 Batch  527/2159   train_loss = 4.530\n",
      "Epoch   7 Batch  537/2159   train_loss = 4.827\n",
      "Epoch   7 Batch  547/2159   train_loss = 4.529\n",
      "Epoch   7 Batch  557/2159   train_loss = 5.216\n",
      "Epoch   7 Batch  567/2159   train_loss = 5.491\n",
      "Epoch   7 Batch  577/2159   train_loss = 4.993\n",
      "Epoch   7 Batch  587/2159   train_loss = 4.156\n",
      "Epoch   7 Batch  597/2159   train_loss = 4.819\n",
      "Epoch   7 Batch  607/2159   train_loss = 4.300\n",
      "Epoch   7 Batch  617/2159   train_loss = 5.082\n",
      "Epoch   7 Batch  627/2159   train_loss = 5.381\n",
      "Epoch   7 Batch  637/2159   train_loss = 5.659\n",
      "Epoch   7 Batch  647/2159   train_loss = 4.684\n",
      "Epoch   7 Batch  657/2159   train_loss = 4.064\n",
      "Epoch   7 Batch  667/2159   train_loss = 5.153\n",
      "Epoch   7 Batch  677/2159   train_loss = 6.262\n",
      "Epoch   7 Batch  687/2159   train_loss = 5.263\n",
      "Epoch   7 Batch  697/2159   train_loss = 5.479\n",
      "Epoch   7 Batch  707/2159   train_loss = 5.401\n",
      "Epoch   7 Batch  717/2159   train_loss = 5.139\n",
      "Epoch   7 Batch  727/2159   train_loss = 4.987\n",
      "Epoch   7 Batch  737/2159   train_loss = 5.042\n",
      "Epoch   7 Batch  747/2159   train_loss = 5.861\n",
      "Epoch   7 Batch  757/2159   train_loss = 4.420\n",
      "Epoch   7 Batch  767/2159   train_loss = 4.140\n",
      "Epoch   7 Batch  777/2159   train_loss = 5.217\n",
      "Epoch   7 Batch  787/2159   train_loss = 4.645\n",
      "Epoch   7 Batch  797/2159   train_loss = 3.994\n",
      "Epoch   7 Batch  807/2159   train_loss = 4.986\n",
      "Epoch   7 Batch  817/2159   train_loss = 6.206\n",
      "Epoch   7 Batch  827/2159   train_loss = 4.680\n",
      "Epoch   7 Batch  837/2159   train_loss = 4.634\n",
      "Epoch   7 Batch  847/2159   train_loss = 4.806\n",
      "Epoch   7 Batch  857/2159   train_loss = 5.027\n",
      "Epoch   7 Batch  867/2159   train_loss = 4.777\n",
      "Epoch   7 Batch  877/2159   train_loss = 4.845\n",
      "Epoch   7 Batch  887/2159   train_loss = 5.190\n",
      "Epoch   7 Batch  897/2159   train_loss = 5.024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7 Batch  907/2159   train_loss = 5.556\n",
      "Epoch   7 Batch  917/2159   train_loss = 4.247\n",
      "Epoch   7 Batch  927/2159   train_loss = 5.250\n",
      "Epoch   7 Batch  937/2159   train_loss = 4.411\n",
      "Epoch   7 Batch  947/2159   train_loss = 4.615\n",
      "Epoch   7 Batch  957/2159   train_loss = 4.537\n",
      "Epoch   7 Batch  967/2159   train_loss = 5.543\n",
      "Epoch   7 Batch  977/2159   train_loss = 4.586\n",
      "Epoch   7 Batch  987/2159   train_loss = 4.498\n",
      "Epoch   7 Batch  997/2159   train_loss = 4.359\n",
      "Epoch   7 Batch 1007/2159   train_loss = 4.809\n",
      "Epoch   7 Batch 1017/2159   train_loss = 5.600\n",
      "Epoch   7 Batch 1027/2159   train_loss = 5.033\n",
      "Epoch   7 Batch 1037/2159   train_loss = 5.014\n",
      "Epoch   7 Batch 1047/2159   train_loss = 4.682\n",
      "Epoch   7 Batch 1057/2159   train_loss = 5.363\n",
      "Epoch   7 Batch 1067/2159   train_loss = 5.157\n",
      "Epoch   7 Batch 1077/2159   train_loss = 4.620\n",
      "Epoch   7 Batch 1087/2159   train_loss = 5.147\n",
      "Epoch   7 Batch 1097/2159   train_loss = 5.544\n",
      "Epoch   7 Batch 1107/2159   train_loss = 4.969\n",
      "Epoch   7 Batch 1117/2159   train_loss = 4.334\n",
      "Epoch   7 Batch 1127/2159   train_loss = 4.984\n",
      "Epoch   7 Batch 1137/2159   train_loss = 5.305\n",
      "Epoch   7 Batch 1147/2159   train_loss = 5.547\n",
      "Epoch   7 Batch 1157/2159   train_loss = 5.670\n",
      "Epoch   7 Batch 1167/2159   train_loss = 5.039\n",
      "Epoch   7 Batch 1177/2159   train_loss = 4.559\n",
      "Epoch   7 Batch 1187/2159   train_loss = 3.215\n",
      "Epoch   7 Batch 1197/2159   train_loss = 4.936\n",
      "Epoch   7 Batch 1207/2159   train_loss = 4.756\n",
      "Epoch   7 Batch 1217/2159   train_loss = 4.690\n",
      "Epoch   7 Batch 1227/2159   train_loss = 4.521\n",
      "Epoch   7 Batch 1237/2159   train_loss = 4.747\n",
      "Epoch   7 Batch 1247/2159   train_loss = 5.445\n",
      "Epoch   7 Batch 1257/2159   train_loss = 4.113\n",
      "Epoch   7 Batch 1267/2159   train_loss = 4.973\n",
      "Epoch   7 Batch 1277/2159   train_loss = 4.012\n",
      "Epoch   7 Batch 1287/2159   train_loss = 3.934\n",
      "Epoch   7 Batch 1297/2159   train_loss = 4.834\n",
      "Epoch   7 Batch 1307/2159   train_loss = 4.255\n",
      "Epoch   7 Batch 1317/2159   train_loss = 4.894\n",
      "Epoch   7 Batch 1327/2159   train_loss = 4.370\n",
      "Epoch   7 Batch 1337/2159   train_loss = 4.726\n",
      "Epoch   7 Batch 1347/2159   train_loss = 5.219\n",
      "Epoch   7 Batch 1357/2159   train_loss = 4.363\n",
      "Epoch   7 Batch 1367/2159   train_loss = 5.772\n",
      "Epoch   7 Batch 1377/2159   train_loss = 4.811\n",
      "Epoch   7 Batch 1387/2159   train_loss = 4.795\n",
      "Epoch   7 Batch 1397/2159   train_loss = 5.092\n",
      "Epoch   7 Batch 1407/2159   train_loss = 5.315\n",
      "Epoch   7 Batch 1417/2159   train_loss = 5.573\n",
      "Epoch   7 Batch 1427/2159   train_loss = 5.194\n",
      "Epoch   7 Batch 1437/2159   train_loss = 5.417\n",
      "Epoch   7 Batch 1447/2159   train_loss = 4.857\n",
      "Epoch   7 Batch 1457/2159   train_loss = 4.197\n",
      "Epoch   7 Batch 1467/2159   train_loss = 4.677\n",
      "Epoch   7 Batch 1477/2159   train_loss = 5.061\n",
      "Epoch   7 Batch 1487/2159   train_loss = 4.822\n",
      "Epoch   7 Batch 1497/2159   train_loss = 4.940\n",
      "Epoch   7 Batch 1507/2159   train_loss = 5.132\n",
      "Epoch   7 Batch 1517/2159   train_loss = 5.248\n",
      "Epoch   7 Batch 1527/2159   train_loss = 4.429\n",
      "Epoch   7 Batch 1537/2159   train_loss = 4.996\n",
      "Epoch   7 Batch 1547/2159   train_loss = 5.466\n",
      "Epoch   7 Batch 1557/2159   train_loss = 4.775\n",
      "Epoch   7 Batch 1567/2159   train_loss = 4.739\n",
      "Epoch   7 Batch 1577/2159   train_loss = 3.735\n",
      "Epoch   7 Batch 1587/2159   train_loss = 4.346\n",
      "Epoch   7 Batch 1597/2159   train_loss = 4.452\n",
      "Epoch   7 Batch 1607/2159   train_loss = 5.078\n",
      "Epoch   7 Batch 1617/2159   train_loss = 4.667\n",
      "Epoch   7 Batch 1627/2159   train_loss = 5.645\n",
      "Epoch   7 Batch 1637/2159   train_loss = 4.990\n",
      "Epoch   7 Batch 1647/2159   train_loss = 4.786\n",
      "Epoch   7 Batch 1657/2159   train_loss = 4.685\n",
      "Epoch   7 Batch 1667/2159   train_loss = 4.259\n",
      "Epoch   7 Batch 1677/2159   train_loss = 4.811\n",
      "Epoch   7 Batch 1687/2159   train_loss = 5.315\n",
      "Epoch   7 Batch 1697/2159   train_loss = 4.690\n",
      "Epoch   7 Batch 1707/2159   train_loss = 4.778\n",
      "Epoch   7 Batch 1717/2159   train_loss = 4.081\n",
      "Epoch   7 Batch 1727/2159   train_loss = 5.523\n",
      "Epoch   7 Batch 1737/2159   train_loss = 4.612\n",
      "Epoch   7 Batch 1747/2159   train_loss = 5.774\n",
      "Epoch   7 Batch 1757/2159   train_loss = 4.482\n",
      "Epoch   7 Batch 1767/2159   train_loss = 4.725\n",
      "Epoch   7 Batch 1777/2159   train_loss = 4.531\n",
      "Epoch   7 Batch 1787/2159   train_loss = 5.871\n",
      "Epoch   7 Batch 1797/2159   train_loss = 5.322\n",
      "Epoch   7 Batch 1807/2159   train_loss = 4.650\n",
      "Epoch   7 Batch 1817/2159   train_loss = 4.765\n",
      "Epoch   7 Batch 1827/2159   train_loss = 5.047\n",
      "Epoch   7 Batch 1837/2159   train_loss = 4.728\n",
      "Epoch   7 Batch 1847/2159   train_loss = 5.134\n",
      "Epoch   7 Batch 1857/2159   train_loss = 5.000\n",
      "Epoch   7 Batch 1867/2159   train_loss = 4.417\n",
      "Epoch   7 Batch 1877/2159   train_loss = 3.845\n",
      "Epoch   7 Batch 1887/2159   train_loss = 5.233\n",
      "Epoch   7 Batch 1897/2159   train_loss = 4.438\n",
      "Epoch   7 Batch 1907/2159   train_loss = 5.230\n",
      "Epoch   7 Batch 1917/2159   train_loss = 4.692\n",
      "Epoch   7 Batch 1927/2159   train_loss = 5.819\n",
      "Epoch   7 Batch 1937/2159   train_loss = 4.636\n",
      "Epoch   7 Batch 1947/2159   train_loss = 4.522\n",
      "Epoch   7 Batch 1957/2159   train_loss = 4.051\n",
      "Epoch   7 Batch 1967/2159   train_loss = 5.600\n",
      "Epoch   7 Batch 1977/2159   train_loss = 4.065\n",
      "Epoch   7 Batch 1987/2159   train_loss = 4.474\n",
      "Epoch   7 Batch 1997/2159   train_loss = 4.537\n",
      "Epoch   7 Batch 2007/2159   train_loss = 4.293\n",
      "Epoch   7 Batch 2017/2159   train_loss = 4.738\n",
      "Epoch   7 Batch 2027/2159   train_loss = 4.633\n",
      "Epoch   7 Batch 2037/2159   train_loss = 4.807\n",
      "Epoch   7 Batch 2047/2159   train_loss = 3.914\n",
      "Epoch   7 Batch 2057/2159   train_loss = 4.869\n",
      "Epoch   7 Batch 2067/2159   train_loss = 4.197\n",
      "Epoch   7 Batch 2077/2159   train_loss = 5.186\n",
      "Epoch   7 Batch 2087/2159   train_loss = 4.415\n",
      "Epoch   7 Batch 2097/2159   train_loss = 4.073\n",
      "Epoch   7 Batch 2107/2159   train_loss = 5.014\n",
      "Epoch   7 Batch 2117/2159   train_loss = 3.307\n",
      "Epoch   7 Batch 2127/2159   train_loss = 5.256\n",
      "Epoch   7 Batch 2137/2159   train_loss = 5.005\n",
      "Epoch   7 Batch 2147/2159   train_loss = 4.906\n",
      "Epoch   7 Batch 2157/2159   train_loss = 4.205\n",
      "Epoch   8 Batch    8/2159   train_loss = 4.442\n",
      "Epoch   8 Batch   18/2159   train_loss = 5.068\n",
      "Epoch   8 Batch   28/2159   train_loss = 5.463\n",
      "Epoch   8 Batch   38/2159   train_loss = 4.946\n",
      "Epoch   8 Batch   48/2159   train_loss = 5.076\n",
      "Epoch   8 Batch   58/2159   train_loss = 4.037\n",
      "Epoch   8 Batch   68/2159   train_loss = 3.845\n",
      "Epoch   8 Batch   78/2159   train_loss = 3.872\n",
      "Epoch   8 Batch   88/2159   train_loss = 5.541\n",
      "Epoch   8 Batch   98/2159   train_loss = 4.816\n",
      "Epoch   8 Batch  108/2159   train_loss = 4.972\n",
      "Epoch   8 Batch  118/2159   train_loss = 4.805\n",
      "Epoch   8 Batch  128/2159   train_loss = 4.447\n",
      "Epoch   8 Batch  138/2159   train_loss = 4.960\n",
      "Epoch   8 Batch  148/2159   train_loss = 4.446\n",
      "Epoch   8 Batch  158/2159   train_loss = 4.489\n",
      "Epoch   8 Batch  168/2159   train_loss = 4.378\n",
      "Epoch   8 Batch  178/2159   train_loss = 4.805\n",
      "Epoch   8 Batch  188/2159   train_loss = 4.035\n",
      "Epoch   8 Batch  198/2159   train_loss = 5.138\n",
      "Epoch   8 Batch  208/2159   train_loss = 4.931\n",
      "Epoch   8 Batch  218/2159   train_loss = 5.143\n",
      "Epoch   8 Batch  228/2159   train_loss = 4.044\n",
      "Epoch   8 Batch  238/2159   train_loss = 5.857\n",
      "Epoch   8 Batch  248/2159   train_loss = 5.077\n",
      "Epoch   8 Batch  258/2159   train_loss = 5.137\n",
      "Epoch   8 Batch  268/2159   train_loss = 4.772\n",
      "Epoch   8 Batch  278/2159   train_loss = 4.879\n",
      "Epoch   8 Batch  288/2159   train_loss = 4.736\n",
      "Epoch   8 Batch  298/2159   train_loss = 4.744\n",
      "Epoch   8 Batch  308/2159   train_loss = 4.600\n",
      "Epoch   8 Batch  318/2159   train_loss = 4.086\n",
      "Epoch   8 Batch  328/2159   train_loss = 4.337\n",
      "Epoch   8 Batch  338/2159   train_loss = 4.940\n",
      "Epoch   8 Batch  348/2159   train_loss = 4.878\n",
      "Epoch   8 Batch  358/2159   train_loss = 4.182\n",
      "Epoch   8 Batch  368/2159   train_loss = 4.950\n",
      "Epoch   8 Batch  378/2159   train_loss = 4.310\n",
      "Epoch   8 Batch  388/2159   train_loss = 3.611\n",
      "Epoch   8 Batch  398/2159   train_loss = 4.132\n",
      "Epoch   8 Batch  408/2159   train_loss = 5.174\n",
      "Epoch   8 Batch  418/2159   train_loss = 5.659\n",
      "Epoch   8 Batch  428/2159   train_loss = 5.548\n",
      "Epoch   8 Batch  438/2159   train_loss = 4.377\n",
      "Epoch   8 Batch  448/2159   train_loss = 5.406\n",
      "Epoch   8 Batch  458/2159   train_loss = 4.658\n",
      "Epoch   8 Batch  468/2159   train_loss = 5.502\n",
      "Epoch   8 Batch  478/2159   train_loss = 5.224\n",
      "Epoch   8 Batch  488/2159   train_loss = 4.700\n",
      "Epoch   8 Batch  498/2159   train_loss = 5.173\n",
      "Epoch   8 Batch  508/2159   train_loss = 5.333\n",
      "Epoch   8 Batch  518/2159   train_loss = 4.594\n",
      "Epoch   8 Batch  528/2159   train_loss = 4.614\n",
      "Epoch   8 Batch  538/2159   train_loss = 4.618\n",
      "Epoch   8 Batch  548/2159   train_loss = 4.656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   8 Batch  558/2159   train_loss = 4.699\n",
      "Epoch   8 Batch  568/2159   train_loss = 4.194\n",
      "Epoch   8 Batch  578/2159   train_loss = 4.791\n",
      "Epoch   8 Batch  588/2159   train_loss = 4.478\n",
      "Epoch   8 Batch  598/2159   train_loss = 4.648\n",
      "Epoch   8 Batch  608/2159   train_loss = 4.757\n",
      "Epoch   8 Batch  618/2159   train_loss = 4.227\n",
      "Epoch   8 Batch  628/2159   train_loss = 4.268\n",
      "Epoch   8 Batch  638/2159   train_loss = 4.666\n",
      "Epoch   8 Batch  648/2159   train_loss = 5.029\n",
      "Epoch   8 Batch  658/2159   train_loss = 4.225\n",
      "Epoch   8 Batch  668/2159   train_loss = 5.814\n",
      "Epoch   8 Batch  678/2159   train_loss = 5.145\n",
      "Epoch   8 Batch  688/2159   train_loss = 4.478\n",
      "Epoch   8 Batch  698/2159   train_loss = 4.487\n",
      "Epoch   8 Batch  708/2159   train_loss = 4.456\n",
      "Epoch   8 Batch  718/2159   train_loss = 4.403\n",
      "Epoch   8 Batch  728/2159   train_loss = 4.506\n",
      "Epoch   8 Batch  738/2159   train_loss = 4.957\n",
      "Epoch   8 Batch  748/2159   train_loss = 4.341\n",
      "Epoch   8 Batch  758/2159   train_loss = 5.119\n",
      "Epoch   8 Batch  768/2159   train_loss = 4.451\n",
      "Epoch   8 Batch  778/2159   train_loss = 5.894\n",
      "Epoch   8 Batch  788/2159   train_loss = 4.512\n",
      "Epoch   8 Batch  798/2159   train_loss = 4.523\n",
      "Epoch   8 Batch  808/2159   train_loss = 5.375\n",
      "Epoch   8 Batch  818/2159   train_loss = 4.246\n",
      "Epoch   8 Batch  828/2159   train_loss = 4.657\n",
      "Epoch   8 Batch  838/2159   train_loss = 4.162\n",
      "Epoch   8 Batch  848/2159   train_loss = 4.920\n",
      "Epoch   8 Batch  858/2159   train_loss = 4.827\n",
      "Epoch   8 Batch  868/2159   train_loss = 4.089\n",
      "Epoch   8 Batch  878/2159   train_loss = 4.924\n",
      "Epoch   8 Batch  888/2159   train_loss = 4.359\n",
      "Epoch   8 Batch  898/2159   train_loss = 4.471\n",
      "Epoch   8 Batch  908/2159   train_loss = 5.022\n",
      "Epoch   8 Batch  918/2159   train_loss = 4.091\n",
      "Epoch   8 Batch  928/2159   train_loss = 4.133\n",
      "Epoch   8 Batch  938/2159   train_loss = 5.509\n",
      "Epoch   8 Batch  948/2159   train_loss = 4.171\n",
      "Epoch   8 Batch  958/2159   train_loss = 4.701\n",
      "Epoch   8 Batch  968/2159   train_loss = 4.669\n",
      "Epoch   8 Batch  978/2159   train_loss = 4.491\n",
      "Epoch   8 Batch  988/2159   train_loss = 4.813\n",
      "Epoch   8 Batch  998/2159   train_loss = 4.649\n",
      "Epoch   8 Batch 1008/2159   train_loss = 4.689\n",
      "Epoch   8 Batch 1018/2159   train_loss = 5.358\n",
      "Epoch   8 Batch 1028/2159   train_loss = 4.616\n",
      "Epoch   8 Batch 1038/2159   train_loss = 4.136\n",
      "Epoch   8 Batch 1048/2159   train_loss = 4.348\n",
      "Epoch   8 Batch 1058/2159   train_loss = 4.365\n",
      "Epoch   8 Batch 1068/2159   train_loss = 4.718\n",
      "Epoch   8 Batch 1078/2159   train_loss = 4.519\n",
      "Epoch   8 Batch 1088/2159   train_loss = 5.503\n",
      "Epoch   8 Batch 1098/2159   train_loss = 5.075\n",
      "Epoch   8 Batch 1108/2159   train_loss = 4.490\n",
      "Epoch   8 Batch 1118/2159   train_loss = 5.211\n",
      "Epoch   8 Batch 1128/2159   train_loss = 4.666\n",
      "Epoch   8 Batch 1138/2159   train_loss = 4.443\n",
      "Epoch   8 Batch 1148/2159   train_loss = 4.538\n",
      "Epoch   8 Batch 1158/2159   train_loss = 5.289\n",
      "Epoch   8 Batch 1168/2159   train_loss = 5.557\n",
      "Epoch   8 Batch 1178/2159   train_loss = 3.833\n",
      "Epoch   8 Batch 1188/2159   train_loss = 4.960\n",
      "Epoch   8 Batch 1198/2159   train_loss = 4.078\n",
      "Epoch   8 Batch 1208/2159   train_loss = 4.862\n",
      "Epoch   8 Batch 1218/2159   train_loss = 4.328\n",
      "Epoch   8 Batch 1228/2159   train_loss = 4.959\n",
      "Epoch   8 Batch 1238/2159   train_loss = 5.176\n",
      "Epoch   8 Batch 1248/2159   train_loss = 4.762\n",
      "Epoch   8 Batch 1258/2159   train_loss = 5.677\n",
      "Epoch   8 Batch 1268/2159   train_loss = 4.929\n",
      "Epoch   8 Batch 1278/2159   train_loss = 4.512\n",
      "Epoch   8 Batch 1288/2159   train_loss = 6.160\n",
      "Epoch   8 Batch 1298/2159   train_loss = 4.826\n",
      "Epoch   8 Batch 1308/2159   train_loss = 4.329\n",
      "Epoch   8 Batch 1318/2159   train_loss = 4.701\n",
      "Epoch   8 Batch 1328/2159   train_loss = 4.780\n",
      "Epoch   8 Batch 1338/2159   train_loss = 4.335\n",
      "Epoch   8 Batch 1348/2159   train_loss = 4.715\n",
      "Epoch   8 Batch 1358/2159   train_loss = 4.225\n",
      "Epoch   8 Batch 1368/2159   train_loss = 4.431\n",
      "Epoch   8 Batch 1378/2159   train_loss = 4.932\n",
      "Epoch   8 Batch 1388/2159   train_loss = 5.072\n",
      "Epoch   8 Batch 1398/2159   train_loss = 4.817\n",
      "Epoch   8 Batch 1408/2159   train_loss = 5.545\n",
      "Epoch   8 Batch 1418/2159   train_loss = 4.967\n",
      "Epoch   8 Batch 1428/2159   train_loss = 4.969\n",
      "Epoch   8 Batch 1438/2159   train_loss = 4.807\n",
      "Epoch   8 Batch 1448/2159   train_loss = 4.815\n",
      "Epoch   8 Batch 1458/2159   train_loss = 4.651\n",
      "Epoch   8 Batch 1468/2159   train_loss = 3.887\n",
      "Epoch   8 Batch 1478/2159   train_loss = 4.775\n",
      "Epoch   8 Batch 1488/2159   train_loss = 4.976\n",
      "Epoch   8 Batch 1498/2159   train_loss = 4.406\n",
      "Epoch   8 Batch 1508/2159   train_loss = 4.975\n",
      "Epoch   8 Batch 1518/2159   train_loss = 5.435\n",
      "Epoch   8 Batch 1528/2159   train_loss = 5.452\n",
      "Epoch   8 Batch 1538/2159   train_loss = 3.969\n",
      "Epoch   8 Batch 1548/2159   train_loss = 4.068\n",
      "Epoch   8 Batch 1558/2159   train_loss = 5.027\n",
      "Epoch   8 Batch 1568/2159   train_loss = 4.685\n",
      "Epoch   8 Batch 1578/2159   train_loss = 4.869\n",
      "Epoch   8 Batch 1588/2159   train_loss = 5.176\n",
      "Epoch   8 Batch 1598/2159   train_loss = 4.801\n",
      "Epoch   8 Batch 1608/2159   train_loss = 5.587\n",
      "Epoch   8 Batch 1618/2159   train_loss = 4.518\n",
      "Epoch   8 Batch 1628/2159   train_loss = 6.406\n",
      "Epoch   8 Batch 1638/2159   train_loss = 4.661\n",
      "Epoch   8 Batch 1648/2159   train_loss = 5.307\n",
      "Epoch   8 Batch 1658/2159   train_loss = 4.504\n",
      "Epoch   8 Batch 1668/2159   train_loss = 5.199\n",
      "Epoch   8 Batch 1678/2159   train_loss = 3.906\n",
      "Epoch   8 Batch 1688/2159   train_loss = 5.836\n",
      "Epoch   8 Batch 1698/2159   train_loss = 5.657\n",
      "Epoch   8 Batch 1708/2159   train_loss = 4.853\n",
      "Epoch   8 Batch 1718/2159   train_loss = 5.271\n",
      "Epoch   8 Batch 1728/2159   train_loss = 5.170\n",
      "Epoch   8 Batch 1738/2159   train_loss = 4.599\n",
      "Epoch   8 Batch 1748/2159   train_loss = 4.130\n",
      "Epoch   8 Batch 1758/2159   train_loss = 5.952\n",
      "Epoch   8 Batch 1768/2159   train_loss = 4.853\n",
      "Epoch   8 Batch 1778/2159   train_loss = 4.480\n",
      "Epoch   8 Batch 1788/2159   train_loss = 5.083\n",
      "Epoch   8 Batch 1798/2159   train_loss = 5.115\n",
      "Epoch   8 Batch 1808/2159   train_loss = 4.750\n",
      "Epoch   8 Batch 1818/2159   train_loss = 5.220\n",
      "Epoch   8 Batch 1828/2159   train_loss = 4.898\n",
      "Epoch   8 Batch 1838/2159   train_loss = 4.945\n",
      "Epoch   8 Batch 1848/2159   train_loss = 5.004\n",
      "Epoch   8 Batch 1858/2159   train_loss = 4.184\n",
      "Epoch   8 Batch 1868/2159   train_loss = 5.273\n",
      "Epoch   8 Batch 1878/2159   train_loss = 4.318\n",
      "Epoch   8 Batch 1888/2159   train_loss = 4.977\n",
      "Epoch   8 Batch 1898/2159   train_loss = 5.904\n",
      "Epoch   8 Batch 1908/2159   train_loss = 4.573\n",
      "Epoch   8 Batch 1918/2159   train_loss = 5.095\n",
      "Epoch   8 Batch 1928/2159   train_loss = 5.073\n",
      "Epoch   8 Batch 1938/2159   train_loss = 4.387\n",
      "Epoch   8 Batch 1948/2159   train_loss = 4.733\n",
      "Epoch   8 Batch 1958/2159   train_loss = 4.178\n",
      "Epoch   8 Batch 1968/2159   train_loss = 3.827\n",
      "Epoch   8 Batch 1978/2159   train_loss = 4.933\n",
      "Epoch   8 Batch 1988/2159   train_loss = 4.444\n",
      "Epoch   8 Batch 1998/2159   train_loss = 4.821\n",
      "Epoch   8 Batch 2008/2159   train_loss = 4.325\n",
      "Epoch   8 Batch 2018/2159   train_loss = 4.513\n",
      "Epoch   8 Batch 2028/2159   train_loss = 4.214\n",
      "Epoch   8 Batch 2038/2159   train_loss = 5.120\n",
      "Epoch   8 Batch 2048/2159   train_loss = 4.523\n",
      "Epoch   8 Batch 2058/2159   train_loss = 5.049\n",
      "Epoch   8 Batch 2068/2159   train_loss = 5.827\n",
      "Epoch   8 Batch 2078/2159   train_loss = 3.943\n",
      "Epoch   8 Batch 2088/2159   train_loss = 3.711\n",
      "Epoch   8 Batch 2098/2159   train_loss = 4.038\n",
      "Epoch   8 Batch 2108/2159   train_loss = 5.736\n",
      "Epoch   8 Batch 2118/2159   train_loss = 6.030\n",
      "Epoch   8 Batch 2128/2159   train_loss = 4.571\n",
      "Epoch   8 Batch 2138/2159   train_loss = 5.218\n",
      "Epoch   8 Batch 2148/2159   train_loss = 3.767\n",
      "Epoch   8 Batch 2158/2159   train_loss = 4.202\n",
      "Epoch   9 Batch    9/2159   train_loss = 4.751\n",
      "Epoch   9 Batch   19/2159   train_loss = 5.173\n",
      "Epoch   9 Batch   29/2159   train_loss = 5.077\n",
      "Epoch   9 Batch   39/2159   train_loss = 5.643\n",
      "Epoch   9 Batch   49/2159   train_loss = 3.919\n",
      "Epoch   9 Batch   59/2159   train_loss = 5.470\n",
      "Epoch   9 Batch   69/2159   train_loss = 4.585\n",
      "Epoch   9 Batch   79/2159   train_loss = 5.815\n",
      "Epoch   9 Batch   89/2159   train_loss = 5.204\n",
      "Epoch   9 Batch   99/2159   train_loss = 4.603\n",
      "Epoch   9 Batch  109/2159   train_loss = 5.230\n",
      "Epoch   9 Batch  119/2159   train_loss = 5.082\n",
      "Epoch   9 Batch  129/2159   train_loss = 4.712\n",
      "Epoch   9 Batch  139/2159   train_loss = 4.691\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9 Batch  149/2159   train_loss = 4.948\n",
      "Epoch   9 Batch  159/2159   train_loss = 4.157\n",
      "Epoch   9 Batch  169/2159   train_loss = 4.879\n",
      "Epoch   9 Batch  179/2159   train_loss = 4.947\n",
      "Epoch   9 Batch  189/2159   train_loss = 5.222\n",
      "Epoch   9 Batch  199/2159   train_loss = 4.560\n",
      "Epoch   9 Batch  209/2159   train_loss = 5.285\n",
      "Epoch   9 Batch  219/2159   train_loss = 4.841\n",
      "Epoch   9 Batch  229/2159   train_loss = 4.813\n",
      "Epoch   9 Batch  239/2159   train_loss = 4.454\n",
      "Epoch   9 Batch  249/2159   train_loss = 4.491\n",
      "Epoch   9 Batch  259/2159   train_loss = 5.090\n",
      "Epoch   9 Batch  269/2159   train_loss = 4.936\n",
      "Epoch   9 Batch  279/2159   train_loss = 4.099\n",
      "Epoch   9 Batch  289/2159   train_loss = 5.053\n",
      "Epoch   9 Batch  299/2159   train_loss = 5.307\n",
      "Epoch   9 Batch  309/2159   train_loss = 4.831\n",
      "Epoch   9 Batch  319/2159   train_loss = 4.944\n",
      "Epoch   9 Batch  329/2159   train_loss = 5.387\n",
      "Epoch   9 Batch  339/2159   train_loss = 5.338\n",
      "Epoch   9 Batch  349/2159   train_loss = 5.574\n",
      "Epoch   9 Batch  359/2159   train_loss = 5.706\n",
      "Epoch   9 Batch  369/2159   train_loss = 5.141\n",
      "Epoch   9 Batch  379/2159   train_loss = 4.597\n",
      "Epoch   9 Batch  389/2159   train_loss = 5.610\n",
      "Epoch   9 Batch  399/2159   train_loss = 5.461\n",
      "Epoch   9 Batch  409/2159   train_loss = 5.302\n",
      "Epoch   9 Batch  419/2159   train_loss = 5.606\n",
      "Epoch   9 Batch  429/2159   train_loss = 4.251\n",
      "Epoch   9 Batch  439/2159   train_loss = 5.292\n",
      "Epoch   9 Batch  449/2159   train_loss = 4.482\n",
      "Epoch   9 Batch  459/2159   train_loss = 5.129\n",
      "Epoch   9 Batch  469/2159   train_loss = 5.589\n",
      "Epoch   9 Batch  479/2159   train_loss = 3.964\n",
      "Epoch   9 Batch  489/2159   train_loss = 5.334\n",
      "Epoch   9 Batch  499/2159   train_loss = 4.067\n",
      "Epoch   9 Batch  509/2159   train_loss = 5.142\n",
      "Epoch   9 Batch  519/2159   train_loss = 4.617\n",
      "Epoch   9 Batch  529/2159   train_loss = 5.425\n",
      "Epoch   9 Batch  539/2159   train_loss = 5.144\n",
      "Epoch   9 Batch  549/2159   train_loss = 4.075\n",
      "Epoch   9 Batch  559/2159   train_loss = 4.958\n",
      "Epoch   9 Batch  569/2159   train_loss = 4.102\n",
      "Epoch   9 Batch  579/2159   train_loss = 5.310\n",
      "Epoch   9 Batch  589/2159   train_loss = 4.515\n",
      "Epoch   9 Batch  599/2159   train_loss = 5.522\n",
      "Epoch   9 Batch  609/2159   train_loss = 4.695\n",
      "Epoch   9 Batch  619/2159   train_loss = 3.879\n",
      "Epoch   9 Batch  629/2159   train_loss = 4.691\n",
      "Epoch   9 Batch  639/2159   train_loss = 4.163\n",
      "Epoch   9 Batch  649/2159   train_loss = 4.071\n",
      "Epoch   9 Batch  659/2159   train_loss = 4.473\n",
      "Epoch   9 Batch  669/2159   train_loss = 4.003\n",
      "Epoch   9 Batch  679/2159   train_loss = 3.823\n",
      "Epoch   9 Batch  689/2159   train_loss = 4.800\n",
      "Epoch   9 Batch  699/2159   train_loss = 4.696\n",
      "Epoch   9 Batch  709/2159   train_loss = 4.460\n",
      "Epoch   9 Batch  719/2159   train_loss = 4.779\n",
      "Epoch   9 Batch  729/2159   train_loss = 4.555\n",
      "Epoch   9 Batch  739/2159   train_loss = 5.166\n",
      "Epoch   9 Batch  749/2159   train_loss = 4.992\n",
      "Epoch   9 Batch  759/2159   train_loss = 5.088\n",
      "Epoch   9 Batch  769/2159   train_loss = 4.905\n",
      "Epoch   9 Batch  779/2159   train_loss = 4.540\n",
      "Epoch   9 Batch  789/2159   train_loss = 5.014\n",
      "Epoch   9 Batch  799/2159   train_loss = 4.500\n",
      "Epoch   9 Batch  809/2159   train_loss = 3.910\n",
      "Epoch   9 Batch  819/2159   train_loss = 4.673\n",
      "Epoch   9 Batch  829/2159   train_loss = 4.502\n",
      "Epoch   9 Batch  839/2159   train_loss = 4.438\n",
      "Epoch   9 Batch  849/2159   train_loss = 5.405\n",
      "Epoch   9 Batch  859/2159   train_loss = 5.381\n",
      "Epoch   9 Batch  869/2159   train_loss = 5.370\n",
      "Epoch   9 Batch  879/2159   train_loss = 4.290\n",
      "Epoch   9 Batch  889/2159   train_loss = 6.075\n",
      "Epoch   9 Batch  899/2159   train_loss = 5.370\n",
      "Epoch   9 Batch  909/2159   train_loss = 5.056\n",
      "Epoch   9 Batch  919/2159   train_loss = 4.561\n",
      "Epoch   9 Batch  929/2159   train_loss = 4.252\n",
      "Epoch   9 Batch  939/2159   train_loss = 5.003\n",
      "Epoch   9 Batch  949/2159   train_loss = 4.845\n",
      "Epoch   9 Batch  959/2159   train_loss = 4.893\n",
      "Epoch   9 Batch  969/2159   train_loss = 4.930\n",
      "Epoch   9 Batch  979/2159   train_loss = 4.442\n",
      "Epoch   9 Batch  989/2159   train_loss = 4.815\n",
      "Epoch   9 Batch  999/2159   train_loss = 4.760\n",
      "Epoch   9 Batch 1009/2159   train_loss = 5.348\n",
      "Epoch   9 Batch 1019/2159   train_loss = 4.513\n",
      "Epoch   9 Batch 1029/2159   train_loss = 4.473\n",
      "Epoch   9 Batch 1039/2159   train_loss = 4.232\n",
      "Epoch   9 Batch 1049/2159   train_loss = 4.731\n",
      "Epoch   9 Batch 1059/2159   train_loss = 3.775\n",
      "Epoch   9 Batch 1069/2159   train_loss = 4.137\n",
      "Epoch   9 Batch 1079/2159   train_loss = 3.857\n",
      "Epoch   9 Batch 1089/2159   train_loss = 6.102\n",
      "Epoch   9 Batch 1099/2159   train_loss = 4.858\n",
      "Epoch   9 Batch 1109/2159   train_loss = 4.908\n",
      "Epoch   9 Batch 1119/2159   train_loss = 4.678\n",
      "Epoch   9 Batch 1129/2159   train_loss = 4.756\n",
      "Epoch   9 Batch 1139/2159   train_loss = 5.339\n",
      "Epoch   9 Batch 1149/2159   train_loss = 4.486\n",
      "Epoch   9 Batch 1159/2159   train_loss = 4.869\n",
      "Epoch   9 Batch 1169/2159   train_loss = 4.867\n",
      "Epoch   9 Batch 1179/2159   train_loss = 4.481\n",
      "Epoch   9 Batch 1189/2159   train_loss = 5.676\n",
      "Epoch   9 Batch 1199/2159   train_loss = 4.490\n",
      "Epoch   9 Batch 1209/2159   train_loss = 4.532\n",
      "Epoch   9 Batch 1219/2159   train_loss = 4.466\n",
      "Epoch   9 Batch 1229/2159   train_loss = 4.398\n",
      "Epoch   9 Batch 1239/2159   train_loss = 4.447\n",
      "Epoch   9 Batch 1249/2159   train_loss = 5.023\n",
      "Epoch   9 Batch 1259/2159   train_loss = 4.739\n",
      "Epoch   9 Batch 1269/2159   train_loss = 4.557\n",
      "Epoch   9 Batch 1279/2159   train_loss = 5.607\n",
      "Epoch   9 Batch 1289/2159   train_loss = 5.919\n",
      "Epoch   9 Batch 1299/2159   train_loss = 5.446\n",
      "Epoch   9 Batch 1309/2159   train_loss = 4.841\n",
      "Epoch   9 Batch 1319/2159   train_loss = 5.566\n",
      "Epoch   9 Batch 1329/2159   train_loss = 5.307\n",
      "Epoch   9 Batch 1339/2159   train_loss = 5.418\n",
      "Epoch   9 Batch 1349/2159   train_loss = 4.258\n",
      "Epoch   9 Batch 1359/2159   train_loss = 5.008\n",
      "Epoch   9 Batch 1369/2159   train_loss = 4.677\n",
      "Epoch   9 Batch 1379/2159   train_loss = 4.270\n",
      "Epoch   9 Batch 1389/2159   train_loss = 4.381\n",
      "Epoch   9 Batch 1399/2159   train_loss = 4.872\n",
      "Epoch   9 Batch 1409/2159   train_loss = 4.614\n",
      "Epoch   9 Batch 1419/2159   train_loss = 4.863\n",
      "Epoch   9 Batch 1429/2159   train_loss = 4.562\n",
      "Epoch   9 Batch 1439/2159   train_loss = 4.528\n",
      "Epoch   9 Batch 1449/2159   train_loss = 5.630\n",
      "Epoch   9 Batch 1459/2159   train_loss = 4.374\n",
      "Epoch   9 Batch 1469/2159   train_loss = 5.179\n",
      "Epoch   9 Batch 1479/2159   train_loss = 4.611\n",
      "Epoch   9 Batch 1489/2159   train_loss = 4.970\n",
      "Epoch   9 Batch 1499/2159   train_loss = 5.263\n",
      "Epoch   9 Batch 1509/2159   train_loss = 5.703\n",
      "Epoch   9 Batch 1519/2159   train_loss = 5.524\n",
      "Epoch   9 Batch 1529/2159   train_loss = 4.094\n",
      "Epoch   9 Batch 1539/2159   train_loss = 4.371\n",
      "Epoch   9 Batch 1549/2159   train_loss = 4.570\n",
      "Epoch   9 Batch 1559/2159   train_loss = 4.949\n",
      "Epoch   9 Batch 1569/2159   train_loss = 5.292\n",
      "Epoch   9 Batch 1579/2159   train_loss = 3.709\n",
      "Epoch   9 Batch 1589/2159   train_loss = 4.730\n",
      "Epoch   9 Batch 1599/2159   train_loss = 4.709\n",
      "Epoch   9 Batch 1609/2159   train_loss = 4.201\n",
      "Epoch   9 Batch 1619/2159   train_loss = 4.926\n",
      "Epoch   9 Batch 1629/2159   train_loss = 4.984\n",
      "Epoch   9 Batch 1639/2159   train_loss = 4.963\n",
      "Epoch   9 Batch 1649/2159   train_loss = 4.972\n",
      "Epoch   9 Batch 1659/2159   train_loss = 4.187\n",
      "Epoch   9 Batch 1669/2159   train_loss = 5.580\n",
      "Epoch   9 Batch 1679/2159   train_loss = 4.494\n",
      "Epoch   9 Batch 1689/2159   train_loss = 4.612\n",
      "Epoch   9 Batch 1699/2159   train_loss = 5.089\n",
      "Epoch   9 Batch 1709/2159   train_loss = 4.066\n",
      "Epoch   9 Batch 1719/2159   train_loss = 4.748\n",
      "Epoch   9 Batch 1729/2159   train_loss = 5.224\n",
      "Epoch   9 Batch 1739/2159   train_loss = 5.424\n",
      "Epoch   9 Batch 1749/2159   train_loss = 4.713\n",
      "Epoch   9 Batch 1759/2159   train_loss = 5.084\n",
      "Epoch   9 Batch 1769/2159   train_loss = 5.110\n",
      "Epoch   9 Batch 1779/2159   train_loss = 5.272\n",
      "Epoch   9 Batch 1789/2159   train_loss = 4.789\n",
      "Epoch   9 Batch 1799/2159   train_loss = 4.188\n",
      "Epoch   9 Batch 1809/2159   train_loss = 3.345\n",
      "Epoch   9 Batch 1819/2159   train_loss = 4.655\n",
      "Epoch   9 Batch 1829/2159   train_loss = 5.437\n",
      "Epoch   9 Batch 1839/2159   train_loss = 4.528\n",
      "Epoch   9 Batch 1849/2159   train_loss = 4.916\n",
      "Epoch   9 Batch 1859/2159   train_loss = 3.793\n",
      "Epoch   9 Batch 1869/2159   train_loss = 5.030\n",
      "Epoch   9 Batch 1879/2159   train_loss = 5.122\n",
      "Epoch   9 Batch 1889/2159   train_loss = 5.110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9 Batch 1899/2159   train_loss = 5.063\n",
      "Epoch   9 Batch 1909/2159   train_loss = 5.100\n",
      "Epoch   9 Batch 1919/2159   train_loss = 5.636\n",
      "Epoch   9 Batch 1929/2159   train_loss = 4.956\n",
      "Epoch   9 Batch 1939/2159   train_loss = 4.722\n",
      "Epoch   9 Batch 1949/2159   train_loss = 5.125\n",
      "Epoch   9 Batch 1959/2159   train_loss = 5.052\n",
      "Epoch   9 Batch 1969/2159   train_loss = 4.620\n",
      "Epoch   9 Batch 1979/2159   train_loss = 4.742\n",
      "Epoch   9 Batch 1989/2159   train_loss = 5.326\n",
      "Epoch   9 Batch 1999/2159   train_loss = 4.769\n",
      "Epoch   9 Batch 2009/2159   train_loss = 4.790\n",
      "Epoch   9 Batch 2019/2159   train_loss = 5.105\n",
      "Epoch   9 Batch 2029/2159   train_loss = 4.659\n",
      "Epoch   9 Batch 2039/2159   train_loss = 5.524\n",
      "Epoch   9 Batch 2049/2159   train_loss = 4.344\n",
      "Epoch   9 Batch 2059/2159   train_loss = 4.327\n",
      "Epoch   9 Batch 2069/2159   train_loss = 4.909\n",
      "Epoch   9 Batch 2079/2159   train_loss = 4.737\n",
      "Epoch   9 Batch 2089/2159   train_loss = 5.068\n",
      "Epoch   9 Batch 2099/2159   train_loss = 5.290\n",
      "Epoch   9 Batch 2109/2159   train_loss = 4.705\n",
      "Epoch   9 Batch 2119/2159   train_loss = 3.661\n",
      "Epoch   9 Batch 2129/2159   train_loss = 4.027\n",
      "Epoch   9 Batch 2139/2159   train_loss = 4.828\n",
      "Epoch   9 Batch 2149/2159   train_loss = 4.784\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Parameters\n",
    "Save `seq_length` and `save_dir` for generating a new TV script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Generate Functions\n",
    "### Get Tensors\n",
    "Get tensors from `loaded_graph` using the function [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name).  Get the tensors using the following names:\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "Return the tensors in the following tuple `(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return None, None, None, None\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Word\n",
    "Implement the `pick_word()` function to select the next word using `probabilities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return None\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TV Script\n",
    "This will generate the TV script for you.  Set `gen_length` to the length of TV script you want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_length = 200\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'moe_szyslak'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TV Script is Nonsensical\n",
    "It's ok if the TV script doesn't make any sense.  We trained on less than a megabyte of text.  In order to get good results, you'll have to use a smaller vocabulary or get more data.  Luckly there's more data!  As we mentioned in the begging of this project, this is a subset of [another dataset](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data).  We didn't have you train on all the data, because that would take too long.  However, you are free to train your neural network on all the data.  After you complete the project, of course.\n",
    "# Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_tv_script_generation.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
